{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bc67c5-9de2-4f8c-8496-5a5ae0f68e3a",
   "metadata": {},
   "source": [
    "#### 08: Event-Driven Backtest (T+1 execution, ATR sizing, stops, kill-switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb5967-5fec-44ca-8500-b31056f17c25",
   "metadata": {},
   "source": [
    "\n",
    "This module introduces a deterministic, daily-bar backtester for long/short signals.  \n",
    "It consumes model signals (by ticker & date) and OHLC prices, converts probabilities into risk-based position sizes, manages trades with targets/stops/trailing, and enforces a global kill-switch on deep drawdowns.  \n",
    "It returns an equity curve and a full trade ledger suitable for evaluation and reporting.\n",
    "\n",
    "---\n",
    "\n",
    "## What the backtester does\n",
    "- Executes signals on **T+1** (next day) using the open price (falls back to close if open is missing).\n",
    "- Filters entries by confidence, ATR bounds, and a short cool-down per ticker.\n",
    "- Sizes positions by risk (equity × risk%) divided by a dollar stop distance derived from ATR.\n",
    "- Manages exits daily via:\n",
    "  - Hard stop proxy  \n",
    "  - Trailing stop  \n",
    "  - Fixed stop/target  \n",
    "  - Time exit (5 days)  \n",
    "- Marks to market at each close and applies commissions + slippage on both entry and exit.\n",
    "- **Kill-switch**: if equity drawdown from its running peak ≤ `kill_dd_pct` (default −30%), all positions are closed and a reset is flagged.\n",
    "\n",
    "---\n",
    "\n",
    "## Required inputs\n",
    "\n",
    "### 1. `merged_data` (signals; one row per potential trade)\n",
    "\n",
    "**Required columns**  \n",
    "- `ticker` — string  \n",
    "- `entry_date` — date when the signal becomes eligible to trade; the engine executes on `entry_date + 1` business day  \n",
    "- `signal` — +1 for long, −1 for short  \n",
    "\n",
    "**Optional (recommended)**  \n",
    "- `prob_up` — model probability for the “up” class (used for confidence-weighted sizing)  \n",
    "- `atr` — recent ATR (either in % of price or $ absolute, see notes)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `prices` (daily OHLC)\n",
    "\n",
    "**Expected columns (case-insensitive)**: `date`, `ticker`, `open`, `high`, `low`, `close`.  \n",
    "If `open` is missing, the engine uses `close`.\n",
    "\n",
    "---\n",
    "\n",
    "## Key parameters (defaults)\n",
    "\n",
    "### Capital & risk\n",
    "- `initial_capital = 100_000`\n",
    "- Per-trade risk grows with confidence:  \n",
    "  \\[\n",
    "  \\text{risk\\_pct} \\;=\\; \\text{base\\_risk\\_pct} + \\text{conf}\\,\\big(\\text{max\\_risk\\_pct} - \\text{base\\_risk\\_pct}\\big)\n",
    "  \\]  \n",
    "  where  \n",
    "  \\[\n",
    "  \\text{conf} = \\text{clip}\\!\\left(\\frac{\\text{prob\\_up}-\\text{prob\\_long\\_min}}{1-\\text{prob\\_long\\_min}},\\,0,\\,1\\right)\n",
    "  \\]  \n",
    "- `base_risk_pct = 0.004` (0.4%)  \n",
    "- `max_risk_pct = 0.08` (8%)\n",
    "\n",
    "### Trade management\n",
    "- Target: `target_pct = 0.07` (+7%)  \n",
    "- Stop: `stop_loss_pct = 0.04` (−4%)  \n",
    "- Trailing stop: `trail_mult = 3.5 × ATR` (see notes on units)  \n",
    "- Time exit: force close after 5 days  \n",
    "- Hard stop proxy: exit if price moves ~−9% vs entry (directional)  \n",
    "\n",
    "### Global risk\n",
    "- Kill-switch: `kill_dd_pct = -0.30`  \n",
    "  - If equity drawdown ≤ −30% from peak, close everything and flag `reset=True`\n",
    "\n",
    "### Costs\n",
    "- `commission_pct = 0.001` (10 bps per leg)  \n",
    "- `slippage_pct = 0.005` (50 bps per fill)  \n",
    "\n",
    "**Trade return formula**:  \n",
    "\\[\n",
    "r = \\frac{P_{\\text{exit}}-P_{\\text{entry}}}{P_{\\text{entry}}}\\cdot \\text{direction} \\;-\\; 2\\,(\\text{commission}+\\text{slippage})\n",
    "\\]\n",
    "\n",
    "### Signal & market filters\n",
    "- Minimum confidence: `prob_long_min = 0.63`, `prob_short_min = 0.63`  \n",
    "- ATR bounds (eligibility): `atr_min = 0.20`, `atr_max = 4.00`  \n",
    "- Per-ticker cool-down: ignore a ticker for 5 days after any exit  \n",
    "\n",
    "---\n",
    "\n",
    "## Sizing & execution details\n",
    "- **Stop distance (dollars)** derived from ATR:  \n",
    "  - If `atr < 3`: treated as a percentage → `$ = atr × entry_price`  \n",
    "  - Else: ATR is taken as-is in dollars  \n",
    "- **Shares**:  \n",
    "  \\[\n",
    "  \\text{shares} = \\frac{\\text{equity\\_start}\\times \\text{risk\\_pct}}{\\text{atr\\_abs}}\\times (1-\\text{commission}-\\text{slippage})\n",
    "  \\]\n",
    "- Trades with cost < $5 notional are skipped.  \n",
    "- Trades that don’t fit available cash are skipped.  \n",
    "\n",
    "**Execution timeline (per day)**  \n",
    "1. Manage open positions on today’s close (hard stop, trailing, stop/target, time exit).  \n",
    "2. Compute equity and check kill-switch; close all if triggered.  \n",
    "3. Open new positions for signals with `entry_date == today`, executed at next day open (T+1) if available.  \n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **portfolio** (DataFrame, daily index)  \n",
    "  Columns: `cash`, `positions` (MTM value), `equity`, `reset` (bool kill-switch flag)  \n",
    "\n",
    "- **trades_df** (DataFrame, one row per completed trade)  \n",
    "  Columns:  \n",
    "  `position_id`, `ticker`, `direction`, `entry_date`, `entry_price`, `exit_date`,  \n",
    "  `exit_price`, `return`, `days_held`, `exit_reason`, `shares`, `notional`  \n",
    "\n",
    "- **missing_df**  \n",
    "  Placeholder; can be extended to log missing price events.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical notes & assumptions\n",
    "- **Daily bars only**: “intraday” hard stop is approximated by daily checks (no intraday simulation).  \n",
    "- **Single position per ticker**: no pyramiding or partial exits (for clarity).  \n",
    "- **ATR units**:  \n",
    "  - If in %, convert to $ via `atr × entry_price`  \n",
    "  - Else use as-is  \n",
    "  - Best practice: standardize ATR to a single convention.  \n",
    "- **No look-ahead**: `entry_date` is when the signal becomes eligible; fills occur next trading day.  \n",
    "- **Costs**: applied on both legs; returns in `trades_df['return']` are net of round-trip commission+slippage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3384f7b5-e05f-4166-ad5c-3fcd3eb5d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import make_column_transformer , ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit ,  train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pathlib import Path\n",
    "import os, random, warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, matthews_corrcoef\n",
    "ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path.cwd()\n",
    "DATA_DIR = Path(os.getenv(\"DATA_DIR\", ROOT / \"data\")) \n",
    "def p(file): return DATA_DIR / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0d65b3-4b5c-46b7-9041-ef09f665f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(\n",
    "    merged_data      : pd.DataFrame,\n",
    "    prices           : pd.DataFrame,\n",
    "    *,\n",
    "   \n",
    "    initial_capital  : float = 100_000,\n",
    "    base_risk_pct    : float = 0.004,   \n",
    "    max_risk_pct     : float = 0.08,\n",
    "    target_pct       : float = 0.07,   \n",
    "    stop_loss_pct    : float = 0.04,    \n",
    "    trail_mult       : float = 3.5,     \n",
    "    kill_dd_pct      : float = -0.30,   \n",
    "    \n",
    "    commission_pct   : float = 0.001,   \n",
    "    slippage_pct     : float = 0.005,   \n",
    "   \n",
    "    prob_long_min    : float = 0.63,\n",
    "    prob_short_min   : float = 0.63,\n",
    "    \n",
    "    atr_min          : float = 0.20,\n",
    "    atr_max          : float = 4.00,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "   \n",
    "    TARGET       = target_pct\n",
    "    STOP_LOSS    = stop_loss_pct\n",
    "    COMMISSION   = commission_pct\n",
    "    SLIPPAGE     = slippage_pct\n",
    "    BASE_RISK_PCT, MAX_RISK_PCT = base_risk_pct, max_risk_pct\n",
    "    MAX_DD_PCT   = kill_dd_pct\n",
    "    MIN_ATR      = atr_min\n",
    "\n",
    "   \n",
    "    prices = prices.reset_index()\n",
    "    merged_data[\"entry_date\"] = pd.to_datetime(merged_data[\"entry_date\"]).dt.normalize()\n",
    "\n",
    "   \n",
    "    date_col = next((c for c in [\"date\", \"Date\", \"datetime\", \"timestamp\"] if c in prices.columns), None)\n",
    "    if date_col is None:\n",
    "        raise KeyError(\"Colonna data non trovata in prices!\")\n",
    "\n",
    "    prices[\"date_normalized\"] = pd.to_datetime(prices[date_col]).dt.normalize()\n",
    "\n",
    "    price_cols = {c.lower(): (c if c in prices.columns else c.title())\n",
    "                  for c in [\"open\", \"close\", \"high\", \"low\"]}\n",
    "\n",
    "   \n",
    "    price_map = {\n",
    "        (row[\"date_normalized\"].date(), row[\"ticker\"]): {\n",
    "            \"open\" : row[price_cols[\"open\"]],\n",
    "            \"close\": row[price_cols[\"close\"]],\n",
    "            \"high\" : row[price_cols[\"high\"]],\n",
    "            \"low\"  : row[price_cols[\"low\"]],\n",
    "        }\n",
    "        for _, row in prices.iterrows()\n",
    "    }\n",
    "\n",
    "    all_dates = pd.date_range(\n",
    "        start=merged_data[\"entry_date\"].min(),\n",
    "        end  =merged_data[\"entry_date\"].max() + pd.Timedelta(days=10),\n",
    "        freq =\"D\",\n",
    "    )\n",
    "\n",
    "   \n",
    "    portfolio = pd.DataFrame(index=all_dates, dtype=float)\n",
    "    portfolio[\"cash\"]      = float(initial_capital)\n",
    "    portfolio[\"positions\"] = 0.0\n",
    "    portfolio[\"equity\"]    = float(initial_capital)\n",
    "    portfolio[\"reset\"]     = False        \n",
    "\n",
    "    \n",
    "    def get_price(ts, ticker, pt):\n",
    "        key = (pd.Timestamp(ts).date(), ticker)\n",
    "        return price_map.get(key, {}).get(pt)\n",
    "\n",
    "   \n",
    "    open_positions  = {}\n",
    "    trades          = []\n",
    "    missing_log     = []\n",
    "    last_exit       = {}\n",
    "    ma50_cache      = {}\n",
    "    position_id     = 1\n",
    "\n",
    "    global_peak = initial_capital  \n",
    "\n",
    "    #  PRINCIPAL\n",
    "    for current_date in tqdm(all_dates, desc=\"Back-test\"):\n",
    "        today = current_date.date()\n",
    "\n",
    "        # 1) START OF THE DAY\n",
    "        if current_date == all_dates[0]:\n",
    "            cash = initial_capital\n",
    "        else:\n",
    "            cash = portfolio.at[current_date - pd.Timedelta(days=1), \"cash\"]\n",
    "\n",
    "        positions_value = 0.0                    \n",
    "        kill_triggered  = False\n",
    "\n",
    "        # 2) CLOSING POSITIONS\n",
    "        to_close = []\n",
    "        for ticker, pos in open_positions.items():\n",
    "            entry_price  = pos[\"entry_price\"]\n",
    "            direction    = pos[\"direction\"]      # +1 long, -1 short\n",
    "            days_held    = (today - pos[\"entry_date\"]).days\n",
    "            current_px   = get_price(current_date, ticker, \"close\")\n",
    "\n",
    "            # MISSING VALUES\n",
    "            if current_px is None:\n",
    "                pos[\"missing\"] = pos.get(\"missing\", 0) + 1\n",
    "                if pos[\"missing\"] < 3:\n",
    "                    positions_value += pos[\"shares\"] * pos[\"last_price\"]\n",
    "                    continue                        \n",
    "                current_px = pos[\"last_price\"]      \n",
    "                exit_reason = \"missing_data\"\n",
    "            else:\n",
    "                pos[\"missing\"] = 0\n",
    "                pos[\"last_price\"] = current_px\n",
    "                exit_reason = None\n",
    "\n",
    "            # hard-stop intraday 9%\n",
    "            if exit_reason is None:\n",
    "                hard_stop = 0.09\n",
    "                if (direction == 1 and current_px <= entry_price * (1 - hard_stop)) or \\\n",
    "                   (direction == -1 and current_px >= entry_price * (1 + hard_stop)):\n",
    "                    exit_reason = \"hard_stop\"\n",
    "\n",
    "            # dinamic trailing stop \n",
    "            if exit_reason is None:\n",
    "                atr = pos[\"atr\"]\n",
    "                trail_mult = 3.5\n",
    "                if direction == 1:\n",
    "                    pos[\"max_price\"] = max(pos.get(\"max_price\", entry_price), current_px)\n",
    "                    stop_trail = pos[\"max_price\"] - trail_mult * np.clip(atr, .2, 1.0) * entry_price\n",
    "                    if current_px <= stop_trail:\n",
    "                        exit_reason = \"trail\"\n",
    "                else:\n",
    "                    pos[\"min_price\"] = min(pos.get(\"min_price\", entry_price), current_px)\n",
    "                    stop_trail = pos[\"min_price\"] + trail_mult * atr * entry_price\n",
    "                    if current_px >= stop_trail:\n",
    "                        exit_reason = \"trail\"\n",
    "\n",
    "            # target / stop \n",
    "            if exit_reason is None:\n",
    "                stop_px   = entry_price * (1 - STOP_LOSS*direction)\n",
    "                target_px = entry_price * (1 + TARGET*direction)\n",
    "                if (direction == 1 and current_px <= stop_px) or \\\n",
    "                   (direction == -1 and current_px >= stop_px):\n",
    "                    exit_reason = \"stop\"\n",
    "                elif (direction == 1 and current_px >= target_px) or \\\n",
    "                     (direction == -1 and current_px <= target_px):\n",
    "                    exit_reason = \"target\"\n",
    "\n",
    "            # time-exit \n",
    "            if exit_reason is None and days_held >= 5:\n",
    "                exit_reason = \"time\"\n",
    "\n",
    "            # Closing?\n",
    "            if exit_reason:\n",
    "                proceeds = pos[\"shares\"] * current_px\n",
    "                fees     = proceeds * (COMMISSION + SLIPPAGE)\n",
    "                cash    += proceeds - fees\n",
    "\n",
    "                ret_pct  = (current_px - entry_price) / entry_price * direction \\\n",
    "                           - 2*(COMMISSION+SLIPPAGE)\n",
    "\n",
    "                trades.append({\n",
    "                    \"position_id\": position_id,\n",
    "                    \"ticker\": ticker,\n",
    "                    \"direction\": \"long\" if direction==1 else \"short\",\n",
    "                    \"entry_date\": pos[\"entry_date\"],\n",
    "                    \"entry_price\": entry_price,\n",
    "                    \"exit_date\": today,\n",
    "                    \"exit_price\": current_px,\n",
    "                    \"return\": ret_pct,\n",
    "                    \"days_held\": days_held,\n",
    "                    \"exit_reason\": exit_reason,\n",
    "                    \"shares\": pos[\"shares\"],                      \n",
    "                    \"notional\": pos[\"shares\"] * entry_price,   \n",
    "                })\n",
    "                position_id += 1\n",
    "                last_exit[ticker] = today\n",
    "                to_close.append(ticker)\n",
    "            else:\n",
    "                positions_value += pos[\"shares\"] * current_px\n",
    "\n",
    "        for t in to_close:\n",
    "            del open_positions[t]\n",
    "\n",
    "       \n",
    "        equity_start = cash + positions_value\n",
    "\n",
    "        # 3) kill-switch \n",
    "        dd_pct = (equity_start - global_peak) / global_peak\n",
    "        if dd_pct <= MAX_DD_PCT:\n",
    "            kill_triggered = True\n",
    "            portfolio.at[current_date, \"reset\"] = True\n",
    "\n",
    "            # Forced closure\n",
    "            for t, pos in list(open_positions.items()):\n",
    "                px   = pos[\"last_price\"]\n",
    "                proceeds = pos[\"shares\"] * px\n",
    "                fees     = proceeds * (COMMISSION + SLIPPAGE)\n",
    "                cash    += proceeds - fees\n",
    "                trades.append({\n",
    "                    \"position_id\": position_id,\n",
    "                    \"ticker\": t,\n",
    "                    \"direction\": \"long\" if pos[\"direction\"]==1 else \"short\",\n",
    "                    \"entry_date\": pos[\"entry_date\"],\n",
    "                    \"entry_price\": pos[\"entry_price\"],\n",
    "                    \"exit_date\": today,\n",
    "                    \"exit_price\": px,\n",
    "                    \"return\": (px - pos[\"entry_price\"])/pos[\"entry_price\"]*pos[\"direction\"]\n",
    "                              - 2*(COMMISSION+SLIPPAGE),\n",
    "                    \"days_held\": (today - pos[\"entry_date\"]).days,\n",
    "                    \"shares\": pos[\"shares\"],\n",
    "                    \"notional\": pos[\"shares\"] * pos[\"entry_price\"],\n",
    "                    \n",
    "                    \"exit_reason\": \"kill_switch\",\n",
    "                })\n",
    "                position_id += 1\n",
    "            open_positions.clear()\n",
    "            positions_value = 0.0       \n",
    "\n",
    "        # 4) New positions if not kill\n",
    "        if not kill_triggered:\n",
    "            todays_signals = merged_data[merged_data[\"entry_date\"].dt.date == today]\n",
    "            if not todays_signals.empty:\n",
    "                for _, sig in todays_signals.iterrows():\n",
    "                    ticker      = sig[\"ticker\"]\n",
    "                    signal      = sig[\"signal\"]         \n",
    "                   \n",
    "                    atr         = max(sig.get(\"atr\", np.nan), MIN_ATR)\n",
    "                    prob_up     = sig.get(\"prob_up\", 0.5)\n",
    "                    prob_down   = 1 - prob_up\n",
    "\n",
    "                    # Filters\n",
    "                    if ticker in open_positions:               continue\n",
    "                    if ticker in last_exit and (today - last_exit[ticker]).days < 5: continue\n",
    "                    if np.isnan(atr) or not (atr_min<= atr <= atr_max):                             continue\n",
    "                    if signal == 1 and prob_up < prob_long_min :                          continue\n",
    "                    if signal == -1 and prob_down < prob_short_min:                       continue\n",
    "\n",
    "                    # Size\n",
    "                    exec_date = current_date + pd.Timedelta(days=1)\n",
    "                    if exec_date > all_dates[-1]:\n",
    "                        continue \n",
    "                    entry_price = get_price(exec_date, ticker, \"open\")\n",
    "                    if entry_price is None:\n",
    "                        entry_price = get_price(exec_date, ticker, \"close\")   \n",
    "                    if entry_price is None:\n",
    "                       continue\n",
    "                    atr_raw = max(sig.get(\"atr\", np.nan), MIN_ATR)\n",
    "                    if atr_raw < 3:\n",
    "                       atr_abs = atr_raw * entry_price   \n",
    "                    else:\n",
    "                        atr_abs = atr_raw                 \n",
    "                    stop_dist = atr_abs \n",
    "\n",
    "                         \n",
    "                    \n",
    "                    conf      = max(0, min(1, (prob_up - prob_long_min)/(1 - prob_long_min)))\n",
    "                    risk_pct  = base_risk_pct + conf*(max_risk_pct - base_risk_pct)\n",
    "                    risk_amt  = equity_start * risk_pct\n",
    "                    stop_dist = atr_abs\n",
    "                    shares    = risk_amt / stop_dist * (1-COMMISSION-SLIPPAGE)\n",
    "                    if shares * entry_price < 5:                    continue\n",
    "                    cost      = shares * entry_price * (1+COMMISSION+SLIPPAGE)\n",
    "                    if cost > cash:                                  continue\n",
    "\n",
    "                    # Open\n",
    "                    open_positions[ticker] = {\n",
    "                        \"entry_date\": exec_date.date(),\n",
    "                        \"entry_price\": entry_price,\n",
    "                        \"shares\": shares,\n",
    "                        \"direction\": 1 if signal>0 else -1,\n",
    "                        \"last_price\": entry_price,\n",
    "                        \"atr\": atr_abs,\n",
    "                        \"max_price\": entry_price,\n",
    "                        \"min_price\":entry_price,\n",
    "                    }\n",
    "                    cash -= cost\n",
    "\n",
    "        # 5) End of the day value\n",
    "        positions_value_end = sum(pos[\"shares\"] * get_price(current_date, t, \"close\")\n",
    "                                  if get_price(current_date, t, \"close\") is not None\n",
    "                                  else pos[\"shares\"] * pos[\"last_price\"]\n",
    "                                  for t, pos in open_positions.items())\n",
    "\n",
    "        equity_end   = cash + positions_value_end\n",
    "        global_peak  = max(global_peak, equity_end)   # picco unico\n",
    "\n",
    "        portfolio.loc[current_date, [\"cash\", \"positions\", \"equity\"]] = (\n",
    "            cash, positions_value_end, equity_end\n",
    "        )\n",
    "\n",
    "    portfolio.ffill(inplace=True)\n",
    "    trades_df       = pd.DataFrame(trades)\n",
    "    missing_df      = pd.DataFrame(missing_log)\n",
    "    return portfolio, trades_df, missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797f50b-28ee-4cbf-8ccf-cc7584f72b8d",
   "metadata": {},
   "source": [
    "# Performance Metrics & Reporting\n",
    "\n",
    "This cell adds two helpers to summarize a backtest and plot the equity and drawdown curves:\n",
    "\n",
    "- `calculate_metrics(portfolio, trades_df) -> dict`  \n",
    "- `report_and_plot(metrics)`  \n",
    "\n",
    "They are **model-agnostic**: pass the `portfolio` and `trades_df` returned by your backtester and you’ll get a compact, auditable snapshot.\n",
    "\n",
    "---\n",
    "\n",
    "## `calculate_metrics(...)` — returns a metrics dictionary\n",
    "\n",
    "### Inputs\n",
    "- **portfolio**: DataFrame indexed by date with at least `equity` (and typically `cash`, `positions`, `reset`).  \n",
    "- **trades_df**: trade ledger with a `return` column (per-trade net return, after costs).  \n",
    "\n",
    "### Outputs (dict)\n",
    "- **Equity**: the equity curve (Series) for downstream plotting.  \n",
    "- **Drawdown**: series of drawdowns, computed as  \n",
    " $$\n",
    "\\text{DD}_t = \\frac{P_t - E_t}{P_t}\n",
    "\\qquad\n",
    "\\text{MDD} = \\max_t \\text{DD}_t\n",
    "$$\n",
    "- **TotRet**: total, non-annualized return over the backtest:  \n",
    "  $$\n",
    "\\text{TotRet} = \\frac{E_{\\text{end}}}{E_{\\text{start}}} - 1\n",
    "$$\n",
    "- **CAGR**: annualized geometric return assuming calendar days:  \n",
    " $$\n",
    "\\text{CAGR} = \\left(\\frac{E_{\\text{end}}}{E_{\\text{start}}}\\right)^{\\tfrac{365.25}{\\text{days}}} - 1\n",
    "$$\n",
    "\n",
    "  (Note: CAGR can be misleading for short horizons; prefer **TotRet** for 1-6 month tests.)  \n",
    "\n",
    "- **Sharpe**: daily Sharpe ratio (risk-free = 0), annualized with \\(\\sqrt{252}\\):  \n",
    "  - Daily returns:  \n",
    "   $$\n",
    "r_t = \\frac{E_t}{E_{t-1}} - 1\n",
    "$$\n",
    "  - Formula:  \n",
    "   $$\n",
    "\\text{Sharpe} = \\frac{\\overline{r}}{s_r}\\sqrt{252}\n",
    "$$\n",
    "- **Max DD**: minimum drawdown (most negative point of the drawdown series).  \n",
    "- **Win rate**: share of trades with `return > 0`.  \n",
    "- **Profit factor**:  \n",
    "$$\n",
    "\\text{PF} = \\frac{\\sum_{t} \\max(r_t, 0)}{\\sum_{t} \\max(-r_t, 0)}\n",
    "$$  \n",
    "- **Num trades**: count of executed trades.  \n",
    "\n",
    "### Edge handling\n",
    "- If `portfolio` is empty → returns `{}`.  \n",
    "- `TotRet` guarded if series length = 0 (returns `NaN`).  \n",
    "- If `trades_df` is empty → trade stats become `NaN`/0 as appropriate.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4ef9fb-bcbd-4ce9-9b08-a18c6cf1d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(portfolio: pd.DataFrame,\n",
    "                      trades_df: pd.DataFrame) -> dict:\n",
    "    \n",
    "    if portfolio.empty:\n",
    "        return {}\n",
    "\n",
    "    eq_series = portfolio[\"equity\"].astype(float)\n",
    "\n",
    "   \n",
    "    tot_ret = float(eq_series.iloc[-1] / eq_series.iloc[0] - 1) if len(eq_series) else float(\"nan\")\n",
    "    days = (eq_series.index[-1] - eq_series.index[0]).days\n",
    "    cagr = (eq_series.iloc[-1] / eq_series.iloc[0])**(365.25/days) - 1\n",
    "\n",
    "    \n",
    "    rets   = eq_series.pct_change().fillna(0)\n",
    "    sharpe = rets.mean() / rets.std() * np.sqrt(252) if rets.std() else 0.0\n",
    "\n",
    "   \n",
    "    dd_series = (eq_series - eq_series.cummax()) / eq_series.cummax()\n",
    "    max_dd    = dd_series.min()\n",
    "\n",
    "  \n",
    "    if trades_df.empty:\n",
    "        win_rate = profit_factor = np.nan\n",
    "        num_trades = 0\n",
    "    else:\n",
    "        win_rate = (trades_df[\"return\"] > 0).mean()\n",
    "        gains    = trades_df.loc[trades_df[\"return\"] > 0, \"return\"].sum()\n",
    "        losses   = -trades_df.loc[trades_df[\"return\"] < 0, \"return\"].sum()\n",
    "        profit_factor = gains / losses if losses else np.inf\n",
    "        num_trades   = len(trades_df)\n",
    "\n",
    "    return {\n",
    "        \"Equity\":           eq_series,\n",
    "        \"Drawdown\":         dd_series,\n",
    "        \"TotRet\"  :         tot_ret,\n",
    "        \"Max DD\":           max_dd,\n",
    "        \"CAGR\":             cagr,\n",
    "        \"Sharpe\":           sharpe,\n",
    "        \"Win rate\":         win_rate,\n",
    "        \"Profit factor\":    profit_factor,\n",
    "        \"Num trades\":       num_trades,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54671e7d-8eaf-4676-8c8b-8801bcfb8e17",
   "metadata": {},
   "source": [
    "# Model Pipeline: Forward-Purged CV, Regime-Aware Stacking, and Signal Generation\n",
    "\n",
    "This cell builds a **leakage-aware classification pipeline** that turns daily cross-sectional features into **probabilistic trade signals** suitable for a T+1 event-driven backtest. It implements **forward-purged cross-validation with per-ticker embargo**, a **stacked base model (XGBoost + Random Forest → Logistic Regression)**, and a **regime-aware meta-gate** that blends predictions based on macro conditions. Finally, it evaluates out-of-fold (OOF) performance, runs a **permutation significance test**, computes **AUC by period**, and converts probabilities into **binary long/short signals** under a participation constraint.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Data setup & cleaning\n",
    "\n",
    "- Ensures reproducibility (`PYTHONHASHSEED`, Python, NumPy seeds).\n",
    "- Loads `ML.parquet`, drops rows with `target_hit == 0` (we keep ±1 only), replaces ±∞ with NaN.\n",
    "- Normalizes `date` to midnight and sorts by `date, ticker`.\n",
    "- Defines `FEATURES` (technical, options, NLP, macro) and `REGIME_COLS` (macro subset).\n",
    "- Builds `X` (features) and `y` (labels where `-1→0, 1→1`), keeps only columns with at least one non-NaN value.\n",
    "\n",
    "**Goal:** produce a clean, time-ordered feature matrix and labels with minimal information loss but no look-ahead.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Forward-purged CV with per-ticker embargo\n",
    "\n",
    "**Function:** `build_forward_purged_folds_strict(...)`\n",
    "\n",
    "- Splits the **unique dates** into `n_splits` contiguous test blocks (forward-in-time).\n",
    "- **Training window:** dates before each test block (optionally limited to a lookback window).\n",
    "- **Per-ticker embargo:** for each ticker, removes train rows within ±`embargo_days` around that ticker’s test dates.\n",
    "- Optional global embargo around the test window.\n",
    "- Enforces **minimum train size** and **minimum history**; discards invalid folds and ensures **no future leakage**.\n",
    "\n",
    "**Why:** Cross-sectional daily data often have overlapping signals; embargo + forward purging materially reduces leakage and inflation of OOS metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Preprocessing\n",
    "\n",
    "- For model features: `SimpleImputer(median, add_indicator=True)` + `StandardScaler`.\n",
    "- For regime (macro) features: `SimpleImputer(median)` + `StandardScaler`.\n",
    "- Applied inside scikit-learn `Pipeline`/`ColumnTransformer` to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Base learner: Stacking (XGB + RF → LR)\n",
    "\n",
    "**`make_model(spw)`** returns:\n",
    "- **XGBoost** (hist) with tuned hyperparameters and `scale_pos_weight = neg/pos`.\n",
    "- **RandomForest** (depth-limited).\n",
    "- **StackingClassifier** that feeds both base models’ `predict_proba` into a **Logistic Regression** finalizer.\n",
    "\n",
    "**Rationale:** Combine a strong gradient booster and a bagged tree model; LR stabilizes and calibrates the stacked probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Inner-OOF on the outer-train to learn a regime gate\n",
    "\n",
    "**Function:** `build_inner_oof(X_sub, y_sub, meta_sub, spw)`\n",
    "\n",
    "- Creates **inner forward-purged folds** (with embargo) on the **outer-fold’s training set** to generate **OOF probabilities** `p_tr_oof` **without leakage**.\n",
    "- Computes **log-loss** of the base model vs. a **null** model (p=0.5).  \n",
    "  - `y_meta = 1` if base log-loss < null log-loss (i.e., base model is **useful** in that regime), else `0`.\n",
    "- If strict inner folds fail (too small), it falls back to a 70/30 forward split.\n",
    "\n",
    "**Outcome:** A **binary target** indicating whether the base model tends to be useful under the contemporaneous macro regime.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Outer-fold loop: OOF base predictions + regime-aware blending\n",
    "\n",
    "For each outer fold:\n",
    "1. Train the **base stack** on the outer-train; produce **OOF base** probabilities `p_te` on the outer-test → store in `p_base`.\n",
    "2. Build **inner-OOF** on the outer-train to derive `y_meta`.\n",
    "3. Train a **regime meta-classifier** (Logistic Regression, class-balanced) on standardized macro features `r_cols` to estimate  \n",
    "   `g_te = P(base is useful | regime)`.\n",
    "4. **Blending (soft gate):**  \n",
    "   \\[\n",
    "   p^{final} = 0.5 + g_{te}\\,(p^{base} - 0.5)\n",
    "   \\]\n",
    "   - If `g_te = 0` → return 0.5 (no edge).\n",
    "   - If `g_te = 1` → return base probability.\n",
    "   - (Option exists for a hard on/off gate, but we use soft blending.)\n",
    "\n",
    "The loop logs per-fold AUC for **base** and **regime-aware** predictions. Aggregating over folds yields **OOF vectors** `p_base` and `p_final`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) OOF evaluation\n",
    "\n",
    "`report(tag, y_true, p)` prints:\n",
    "- **AUC**, **Brier score**, **Accuracy** (0.5 threshold), **MCC** (robust to class imbalance), and sample size.\n",
    "\n",
    "Reported for:\n",
    "- `BASE OOF` (stack only)  \n",
    "- `REGIME OOF` (stack + meta-gate)\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Significance test & stability diagnostics\n",
    "\n",
    "- **Permutation test (N=5000):** permutes `y`, recomputes AUC each time, and reports the fraction ≥ observed AUC → **p-value**.\n",
    "- **AUC by period:** computes AUC per **quarter** and **half-year** using `dates_valid`. If a period lacks both classes, AUC=NaN.\n",
    "\n",
    "**Purpose:** Check that performance is **statistically meaningful** and **stable** over time, not driven by a small window.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Threshold selection under participation constraint\n",
    "\n",
    "We convert probabilities into long/short **signals**:\n",
    "\n",
    "- Desired **long participation**: `TARGET_SHARE = 0.50` with tolerance `BAND = 0.05`.\n",
    "- Construct a candidate set of thresholds from a uniform grid and quantiles of the valid probabilities.\n",
    "- For each threshold `t`:\n",
    "  - Compute class share `mean(p ≥ t)`.\n",
    "  - If within the target band, compute **MCC**; keep the best.\n",
    "- **Fallback:** if no `t` satisfies the band, use the `(1 − TARGET_SHARE)` quantile of `p`.\n",
    "\n",
    "**Output columns:**\n",
    "- `prob_up_base` (OOF base; 0.5 where invalid)\n",
    "- `prob_up_final` (OOF regime-aware; 0.5 where invalid)\n",
    "- `prob_down_final = 1 − prob_up_final`\n",
    "- `signal = +1` if `prob_up_final ≥ t*`, else `−1`\n",
    "\n",
    "This `df_signals` is **ready for the event-driven backtester** (T+1 execution, ATR-based sizing, stops, kill-switch).\n",
    "\n",
    "---\n",
    "\n",
    "## Design choices & safeguards\n",
    "\n",
    "- **Leakage control:** forward-purged folds + per-ticker embargo at both outer and inner levels.\n",
    "- **Class imbalance:** `scale_pos_weight` in XGB; class-balanced LR for regime gate.\n",
    "- **Calibration & robustness:** stacking with LR; Brier score; permutation test; AUC by period.\n",
    "- **Operational readiness:** outputs include probabilities and a **directional signal** constrained to the targeted participation rate.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb75a9e-37b1-4ffd-811e-a8b49ae5c76d",
   "metadata": {},
   "source": [
    " #### 🚩 Hyperparameter Policy\n",
    "\n",
    "- **Notebook 07** reports the official **nested forward-purged BayesSearchCV** tuning procedure.  \n",
    "- **Notebook 08** uses a **frozen configuration** to generate signals for backtesting and paper trading.  \n",
    "\n",
    "During exploratory phases (not shown in the project) we observed that a simple GroupKFold (by ticker) preferred:  \n",
    "- slightly **higher** `n_estimators` (XGB/RF),  \n",
    "- a **lower** logistic regression \\(C\\),  \n",
    "- a **lower** XGB \\( \\alpha \\).  \n",
    "\n",
    "These settings yielded **greater fold-to-fold stability** and more consistent realized P&L.  \n",
    "The nested Bayes search (Notebook 07) confirmed broadly similar ranges, but delivered slightly lower OOF AUC (~−0.02) and higher variance.  \n",
    "\n",
    "For **stability in paper trading**, we froze a **hybrid configuration**:  \n",
    "- anchored on the GroupKFold preferences (for robustness),  \n",
    "- consistent with the nested Bayes search ranges.  \n",
    "\n",
    "**Policy to avoid sub-optimization:**\n",
    "1. No retuning during the paper-trading window.  \n",
    "2. Nested Bayes will only be re-run if the **feature set changes** or the sample is extended.  \n",
    "3. All configurations are tracked in versioned YAML with fixed seeds, folds, and embargo settings.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa5280f-b817-477a-91f6-cb7b528a1387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: train  171 | test  356 | AUC base 0.634 | AUC regime 0.367\n",
      "Fold 1: train  526 | test  700 | AUC base 0.533 | AUC regime 0.536\n",
      "Fold 2: train 1101 | test  663 | AUC base 0.517 | AUC regime 0.516\n",
      "Fold 3: train 1619 | test 1074 | AUC base 0.612 | AUC regime 0.595\n",
      "[BASE OOF] AUC 0.560 | Brier 0.247 | ACC 0.546 | MCC 0.084 | n=2793\n",
      "[REGIME OOF] AUC 0.549 | Brier 0.248 | ACC 0.546 | MCC 0.084 | n=2793\n",
      "Permutation test p-value (final): 0.0000\n",
      "\n",
      "AUC per trimestre (final):\n",
      "    period       auc  n_obs\n",
      "0  2024Q2  0.517747     78\n",
      "1  2024Q3  0.446487    405\n",
      "2  2024Q4  0.499288    663\n",
      "3  2025Q1  0.472474    698\n",
      "4  2025Q2  0.595620    949\n",
      "\n",
      " Choice t*=0.480 | long share=56.69%\n"
     ]
    }
   ],
   "source": [
    "df_ml = pd.read_parquet(p('ML.parquet'))\n",
    "df_ml[\"date\"] = pd.to_datetime(df_ml[\"date\"]).dt.normalize()\n",
    "df_ml = df_ml.sort_values([\"date\",\"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "lr_C = 1.6\n",
    "\n",
    "FEATURES = [\n",
    "    \"atr_10d_Tm1\",\"vol_5d_Tm1\",\"momentum_5d_Tm1\",\"finroberta_compound\",\n",
    "    \"opt_total_option_volume\",\"volume_spike_Tm1\",\"cumret_20d_Tm1\",\"maxdd_20d_Tm1\",\n",
    "    \"nlp_logit\",\"nlp_margin\",\"ev_fda_pos\",\"ev_fda_neg\",\"volume_5d\",\"nlp_entropy\",\n",
    "    \"IBB_ret_20d\",\"finroberta_neg\",\"opt_avg_iv_call_ln\",\n",
    "    \"IBB_v_Tm1\",\"spread_3m_10y\",\"VIX_ma5_x\",\"SPY_ma5\",\n",
    "    \"HY_OAS_z\",\"HY_OAS_chg_5d\",\"slope_2s10s\",\"DTWEXBGS\",\"XBI_over_IBB_RS20\",\n",
    "]\n",
    "REGIME_COLS = [\"VIX_ma5_x\",\"HY_OAS_z\",\"HY_OAS_chg_5d\",\"slope_2s10s\",\"DTWEXBGS\",\"XBI_over_IBB_RS20\"]\n",
    "\n",
    "X_full = df_ml[FEATURES]\n",
    "y_full = df_ml[\"target_hit\"].replace({-1:0, 1:1}).astype(int).values\n",
    "\n",
    "row_mask = X_full.notna().any(axis=1).values\n",
    "X = X_full.loc[row_mask].reset_index(drop=True)\n",
    "y = y_full[row_mask]\n",
    "meta = df_ml.loc[row_mask, [\"date\",\"ticker\"] + REGIME_COLS].reset_index(drop=True)\n",
    "\n",
    "good_cols = [c for c in X.columns if X[c].notna().any()]\n",
    "X = X[good_cols]\n",
    "#same of 07_nb\n",
    "def build_forward_purged_folds_strict(\n",
    "    meta_df: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    lookback_days: int | None = 252,\n",
    "    min_train_obs: int = 200,\n",
    "    min_history_days: int = 5,\n",
    "    per_ticker_embargo_days: int = 5,\n",
    "    global_embargo_days: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    d = meta_df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"]).dt.normalize()\n",
    "    d = d.sort_values([\"date\",\"ticker\"]).reset_index(drop=True)\n",
    "    uniq_dates = d[\"date\"].drop_duplicates().sort_values().to_list()\n",
    "    date_blocks = np.array_split(uniq_dates, n_splits)\n",
    "\n",
    "    folds = []\n",
    "    for k, block in enumerate(date_blocks):\n",
    "        test_dates = pd.to_datetime(pd.Index(block)).sort_values()\n",
    "        test_start, test_end = test_dates.min(), test_dates.max()\n",
    "        if (test_start - uniq_dates[0]).days < min_history_days:\n",
    "            if verbose:\n",
    "                print(f\" Fold {k} saltato: no history (<{min_history_days}g).\")\n",
    "            continue\n",
    "\n",
    "        test_mask = d[\"date\"].isin(test_dates).values\n",
    "        t0 = d[\"date\"].min() if lookback_days is None else test_start - pd.Timedelta(days=lookback_days)\n",
    "        base_train = (d[\"date\"] < test_start) & (d[\"date\"] >= t0)\n",
    "\n",
    "        if global_embargo_days is not None and global_embargo_days > 0:\n",
    "            lo_g = test_start - pd.Timedelta(days=global_embargo_days)\n",
    "            hi_g = test_end   + pd.Timedelta(days=global_embargo_days)\n",
    "            base_train &= ~d[\"date\"].between(lo_g, hi_g)\n",
    "\n",
    "        tr = d.loc[base_train, [\"ticker\",\"date\"]].copy()\n",
    "        te = d.loc[test_mask, [\"ticker\",\"date\"]].copy()\n",
    "        if len(tr) == 0 or len(te) == 0:\n",
    "            if verbose:\n",
    "                print(f\" Fold {k} skip: train/test empty.\")\n",
    "            continue\n",
    "\n",
    "        m = tr.merge(te, on=\"ticker\", suffixes=(\"_tr\",\"_te\"))\n",
    "        lo = m[\"date_te\"] - pd.Timedelta(days=per_ticker_embargo_days)\n",
    "        hi = m[\"date_te\"] + pd.Timedelta(days=per_ticker_embargo_days)\n",
    "        clash = m[\"date_tr\"].between(lo, hi)\n",
    "\n",
    "        keep_train = base_train.copy()\n",
    "        if clash.any():\n",
    "            bad = m.loc[clash, [\"ticker\",\"date_tr\"]].drop_duplicates()\n",
    "            bad[\"flag\"] = True\n",
    "            tr2 = tr.merge(bad, left_on=[\"ticker\",\"date\"], right_on=[\"ticker\",\"date_tr\"], how=\"left\")\n",
    "            # Robust boolean mask without fillna downcasting:\n",
    "            drop_mask_local = tr2[\"flag\"].eq(True).to_numpy()\n",
    "            idx_local = np.flatnonzero(base_train)\n",
    "            keep_train[idx_local[drop_mask_local]] = False\n",
    "\n",
    "        if pd.to_datetime(d.loc[keep_train, \"date\"]).max() >= test_start:\n",
    "            if verbose:\n",
    "                print(f\"  Fold {k} skip: train in the future.\")\n",
    "            continue\n",
    "        if keep_train.sum() < min_train_obs:\n",
    "            if verbose:\n",
    "                print(f\"  Fold {k} skip: train too small ({keep_train.sum()} < {min_train_obs}).\")\n",
    "            continue\n",
    "\n",
    "        folds.append((keep_train.values, test_mask))\n",
    "    assert len(folds) >= 2, \"Too much fold skipped.\"\n",
    "    return folds\n",
    "\n",
    "folds = build_forward_purged_folds_strict(\n",
    "    meta[[\"date\",\"ticker\"]],\n",
    "    n_splits=5,\n",
    "    lookback_days=252,\n",
    "    min_train_obs=170,\n",
    "    min_history_days=5,          \n",
    "    per_ticker_embargo_days=5,\n",
    "    global_embargo_days=None,\n",
    "    verbose=False               )\n",
    "\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[(\"num\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "        (\"sc\",  StandardScaler())\n",
    "    ]), good_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "R_FEATURES = [\"VIX_ma5_x\",\"HY_OAS_z\",\"slope_2s10s\",\"DTWEXBGS\",\"XBI_over_IBB_RS20\",\"spread_3m_10y\"]\n",
    "r_cols = [c for c in R_FEATURES if c in X.columns]\n",
    "preproc_regime = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "def make_model(spw: float):\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500, max_depth=5, learning_rate=0.019001692618319327,\n",
    "        subsample=0.6170834796766332, colsample_bytree=0.797503119571276, tree_method=\"hist\",min_child_weight = 6,\n",
    "        gamma = 0.009311515004379921,reg_lambda = 2.3622918650462594, reg_alpha = 0.1,\n",
    "        eval_metric=\"auc\", random_state=45, n_jobs=-1,\n",
    "        scale_pos_weight=spw\n",
    "    )\n",
    "    rf  = RandomForestClassifier(\n",
    "        n_estimators=622, max_depth=11, max_features=0.5176158986513577,\n",
    "        min_samples_leaf=5, min_samples_split=4,\n",
    "        random_state=45, n_jobs=-1\n",
    "    )\n",
    "    stack = StackingClassifier(\n",
    "        estimators=[('xgb', xgb), ('rf', rf)],\n",
    "        final_estimator=LogisticRegression(C=lr_C, penalty='l2', solver='lbfgs', max_iter=2000),\n",
    "        stack_method=\"predict_proba\", cv=3, n_jobs=-1\n",
    "    )\n",
    "    return Pipeline([(\"pre\", preproc), (\"model\", stack)])\n",
    "\n",
    "\n",
    "\n",
    "def build_inner_oof(X_sub, y_sub, meta_sub, spw):\n",
    "    def try_inner(n_splits, lookback_days, min_train_obs, min_history_days, embargo_days):\n",
    "        inner = build_forward_purged_folds_strict(\n",
    "            meta_sub[[\"date\",\"ticker\"]],\n",
    "            n_splits=n_splits,\n",
    "            lookback_days=lookback_days,\n",
    "            min_train_obs=min_train_obs,\n",
    "            min_history_days=min_history_days,\n",
    "            per_ticker_embargo_days=embargo_days,\n",
    "            global_embargo_days=None,\n",
    "            verbose=False  # <— evita stampe durante i tentativi\n",
    "        )\n",
    "        oof_tmp = np.full(len(X_sub), np.nan, dtype=float)\n",
    "        for (tr_m, te_m) in inner:\n",
    "            pipe = make_model(spw)\n",
    "            pipe.fit(X_sub.iloc[tr_m], y_sub[tr_m])\n",
    "            oof_tmp[te_m] = pipe.predict_proba(X_sub.iloc[te_m])[:, 1]\n",
    "        return oof_tmp\n",
    "\n",
    "    n_obs = len(X_sub)\n",
    "    configs = [\n",
    "        dict(n_splits=3, lookback_days=None, min_train_obs=max(100, int(0.12*n_obs)), min_history_days=30, embargo_days=5),\n",
    "        dict(n_splits=2, lookback_days=None, min_train_obs=max(80,  int(0.10*n_obs)), min_history_days=20, embargo_days=3),\n",
    "    ]\n",
    "    for cfg in configs:\n",
    "        try:\n",
    "            oof_sub = try_inner(**cfg)\n",
    "            if np.isfinite(oof_sub).sum() > 0:\n",
    "                return np.where(np.isnan(oof_sub), 0.5, oof_sub)\n",
    "        except AssertionError:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    d = meta_sub.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"]).dt.normalize()\n",
    "    uniq = d[\"date\"].drop_duplicates().sort_values().to_list()\n",
    "    if len(uniq) < 10:\n",
    "        return np.full(n_obs, 0.5, dtype=float)\n",
    "    cut = int(len(uniq)*0.7)\n",
    "    tr_m = meta_sub[\"date\"].isin(uniq[:cut]).values\n",
    "    te_m = meta_sub[\"date\"].isin(uniq[cut:]).values\n",
    "    oof_sub = np.full(n_obs, 0.5, dtype=float)\n",
    "    if tr_m.sum() >= max(60, int(0.08*n_obs)) and te_m.sum() >= 20:\n",
    "        pipe = make_model(spw)\n",
    "        pipe.fit(X_sub.iloc[tr_m], y_sub[tr_m])\n",
    "        oof_sub[te_m] = pipe.predict_proba(X_sub.iloc[te_m])[:, 1]\n",
    "    return oof_sub\n",
    "\n",
    "\n",
    "HARD_GATE = False\n",
    "META_THR  = 0.60\n",
    "eps = 1e-9\n",
    "\n",
    "p_base  = np.full(len(X), np.nan, dtype=float)\n",
    "p_final = np.full(len(X), np.nan, dtype=float)\n",
    "\n",
    "for k, (tr_mask, te_mask) in enumerate(folds):\n",
    "    X_tr, X_te = X.iloc[tr_mask], X.iloc[te_mask]\n",
    "    y_tr, y_te = y[tr_mask], y[te_mask]\n",
    "    meta_tr, meta_te = meta.iloc[tr_mask], meta.iloc[te_mask]\n",
    "\n",
    "    # base OOF \n",
    "    pos = (y_tr == 1).sum(); neg = (y_tr == 0).sum()\n",
    "    spw = neg / max(1, pos)\n",
    "    base = make_model(spw)\n",
    "    base.fit(X_tr, y_tr)\n",
    "    p_te = base.predict_proba(X_te)[:, 1]\n",
    "    p_base[te_mask] = p_te\n",
    "\n",
    "    # inner-OOF \n",
    "    p_tr_oof = build_inner_oof(X_tr, y_tr, meta_tr, spw)\n",
    "    p_tr_clip = np.clip(p_tr_oof, eps, 1 - eps)\n",
    "    ll_base = -(y_tr*np.log(p_tr_clip) + (1 - y_tr)*np.log(1 - p_tr_clip))\n",
    "    ll_null = -np.log(0.5)\n",
    "    y_meta  = (ll_base < ll_null).astype(int)\n",
    "\n",
    "    if len(r_cols) == 0:\n",
    "        p_final[te_mask] = p_te\n",
    "        p_te_f = p_te\n",
    "    else:\n",
    "        Z_tr = preproc_regime.fit_transform(X_tr[r_cols])\n",
    "        Z_te = preproc_regime.transform(X_te[r_cols])\n",
    "        meta_clf = LogisticRegression(C=2.0, solver='lbfgs', max_iter=2000, class_weight='balanced')\n",
    "        meta_clf.fit(Z_tr, y_meta)\n",
    "        g_te = meta_clf.predict_proba(Z_te)[:, 1]\n",
    "\n",
    "        if HARD_GATE:\n",
    "            use = (g_te >= META_THR).astype(float)\n",
    "            p_te_f = 0.5 + use*(p_te - 0.5)  # on/off\n",
    "        else:\n",
    "            p_te_f = 0.5 + g_te*(p_te - 0.5)  # soft blending\n",
    "        p_final[te_mask] = p_te_f\n",
    "\n",
    "    print(f\"Fold {k}: train {tr_mask.sum():4d} | test {te_mask.sum():4d} | \"\n",
    "          f\"AUC base {roc_auc_score(y_te, p_te):.3f} | AUC regime {roc_auc_score(y_te, p_te_f):.3f}\")\n",
    "\n",
    "\n",
    "def report(tag, y_true, p):\n",
    "    v = ~np.isnan(p)\n",
    "    auc   = roc_auc_score(y_true[v], p[v])\n",
    "    brier = brier_score_loss(y_true[v], p[v])\n",
    "    acc   = accuracy_score(y_true[v], (p[v] >= 0.5).astype(int))\n",
    "    mcc   = matthews_corrcoef(y_true[v], (p[v] >= 0.5).astype(int))\n",
    "    print(f\"[{tag}] AUC {auc:.3f} | Brier {brier:.3f} | ACC {acc:.3f} | MCC {mcc:.3f} | n={v.sum()}\")\n",
    "    return auc, v\n",
    "\n",
    "auc_b, vb = report(\"BASE OOF\",   y, p_base)\n",
    "auc_f, vf = report(\"REGIME OOF\", y, p_final)\n",
    "\n",
    "\n",
    "valid_f = vf\n",
    "y_valid, p_valid = y[valid_f], p_final[valid_f]\n",
    "\n",
    "\n",
    "N_PERM = 5000\n",
    "rng = np.random.default_rng(42)\n",
    "ge = sum(roc_auc_score(rng.permutation(y_valid), p_valid) >= auc_f for _ in range(N_PERM))\n",
    "print(f\"Permutation test p-value (final): {ge/N_PERM:.4f}\")\n",
    "\n",
    "dates_valid = meta.loc[valid_f, \"date\"].reset_index(drop=True)\n",
    "\n",
    "def auc_by_period(dates, y, p, freq):\n",
    "    df = pd.DataFrame({\"date\": dates, \"y\": y, \"p\": p})\n",
    "    out = []\n",
    "    for per, g in df.groupby(df[\"date\"].dt.to_period(freq)):\n",
    "        if g[\"y\"].nunique()==2:\n",
    "            out.append((str(per), roc_auc_score(g[\"y\"], g[\"p\"]), len(g)))\n",
    "        else:\n",
    "            out.append((str(per), np.nan, len(g)))\n",
    "    return pd.DataFrame(out, columns=[\"period\",\"auc\",\"n_obs\"]).sort_values(\"period\")\n",
    "\n",
    "print(\"\\nAUC per trimestre (final):\\n\", auc_by_period(dates_valid, y_valid, p_valid, \"Q\"))\n",
    "\n",
    "\n",
    "# Signals\n",
    "TARGET_SHARE, BAND = 0.50, 0.05\n",
    "ths = np.unique(np.concatenate([\n",
    "    np.linspace(0.30, 0.70, 81),\n",
    "    np.quantile(p_valid, np.linspace(0.05, 0.95, 91))\n",
    "]))\n",
    "ths = ths[(ths >= 0.01) & (ths <= 0.99)]\n",
    "\n",
    "def pick_threshold_constrained(p, y, target_share=TARGET_SHARE, band=BAND):\n",
    "    best = -1; t_star = 0.5\n",
    "    for t in ths:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        share = yhat.mean()\n",
    "        if abs(share - target_share) <= band:\n",
    "            mcc = matthews_corrcoef(y, yhat)\n",
    "            if mcc > best:\n",
    "                best, t_star = mcc, t\n",
    "    if best < 0:\n",
    "        return float(np.quantile(p, 1 - target_share))\n",
    "    return float(t_star)\n",
    "\n",
    "t_star = pick_threshold_constrained(p_valid, y_valid)\n",
    "df_signals = df_ml.loc[row_mask].reset_index(drop=True).copy()\n",
    "df_signals[\"prob_up_base\"]  = 0.5; df_signals.loc[vb, \"prob_up_base\"]  = p_base[vb]\n",
    "df_signals[\"prob_up_final\"] = 0.5; df_signals.loc[vf, \"prob_up_final\"] = p_final[vf]\n",
    "df_signals[\"prob_down_final\"] = 1.0 - df_signals[\"prob_up_final\"]\n",
    "df_signals[\"signal\"] = np.where(df_signals[\"prob_up_final\"] >= t_star, 1, -1)\n",
    "print(f\"\\n Choice t*={t_star:.3f} | long share={(df_signals['signal']>0).mean():.2%}\")\n",
    "# df_signals.to_csv(\"trade_signals_OOF_regimeaware.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f6c82-f0d6-4051-ad19-40a7cb215da4",
   "metadata": {},
   "source": [
    "#### Embargo Sensitivity Check\n",
    "\n",
    "We stress-tested the **per-ticker embargo window** by varying it from **1 day** up to **10 days**.  \n",
    "The resulting OOF AUC differed by only ~**0.01/02** on average compared to the baseline (5 days).  \n",
    "\n",
    "This stability indicates that the model is **not materially affected by look-ahead bias or bleed** between training and test samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ab346-5bd6-4c79-9d55-ef4da52c6326",
   "metadata": {},
   "source": [
    "# Adding `meta_prob` to the signals DataFrame\n",
    "\n",
    "In addition to the base model probability (`prob_up_base`) and the regime-aware blended probability (`prob_up_final`),  \n",
    "we also store the **raw output of the meta-classifier** (the regime gate).  \n",
    "\n",
    "---\n",
    "\n",
    "## How it works\n",
    "- We initialize a vector `meta_prob` filled with NaN.  \n",
    "- Inside each outer fold, when we compute `g_te = P(base model is useful | regime features)`,  \n",
    "  we assign these values into the appropriate test slice of `meta_prob`.  \n",
    "- After the loop, when constructing `df_signals`, we add a column:\n",
    "  ```python\n",
    "  df_signals[\"meta_prob\"] = 0.5\n",
    "  df_signals.loc[vf, \"meta_prob\"] = meta_prob[vf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110de876-7316-41ea-b113-11b6913409e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prob = np.full(len(X), np.nan, dtype=float)\n",
    "meta_prob[te_mask] = g_te\n",
    "df_signals[\"meta_prob\"] = 0.5\n",
    "df_signals.loc[vf, \"meta_prob\"] = meta_prob[vf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8848782-7700-422a-8a64-f5849fb87019",
   "metadata": {},
   "source": [
    "# `make_merged(...)` Helper\n",
    "\n",
    "### Inputs\n",
    "- **df_signals**: DataFrame with model outputs, containing at least  \n",
    "  `['date', 'ticker', prob_col]`.\n",
    "- **df_features**: Feature dataset with  \n",
    "  `['date', 'ticker', atr_col]`, so we can bring in ATR values.\n",
    "- **prob_col**: The column in `df_signals` that stores the probability we want to trade on  \n",
    "  (e.g., `prob_up_final` or `meta_prob`).\n",
    "- **t_star**: Decision threshold chosen earlier (constrained to desired long/short share).\n",
    "- **gate_mask** (optional): Boolean mask  \n",
    "  (e.g., `df_signals[\"meta_prob\"] >= τ`) to restrict trades to certain regimes.\n",
    "- **atr_col**: The ATR feature to use for risk sizing (default = `\"atr_10d_Tm1\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### Processing Steps\n",
    "\n",
    "1. **Optional filter**  \n",
    "   - If a `gate_mask` is provided, keep only rows where the condition is `True`  \n",
    "     (for example: restrict to trusted regimes).\n",
    "\n",
    "2. **Rename & normalize**  \n",
    "   - `date → entry_date` (normalized to midnight).  \n",
    "   - The chosen probability column → `prob_up`.\n",
    "\n",
    "3. **Generate directional signal**  \n",
    "   ```python\n",
    "   df[\"signal\"] = np.where(df[\"prob_up\"] >= t_star, 1, -1)\n",
    "### Step 4. Merge ATR values\n",
    "- From `df_features`, bring in `atr_col` aligned by `(entry_date, ticker)`.  \n",
    "- Standardize column name to `\"atr\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5. Finalize\n",
    "- Sort by `(entry_date, ticker)`.  \n",
    "- Remove duplicates.  \n",
    "- Keep only the expected backtest columns.\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "A clean DataFrame with exactly the columns required by the backtester:\n",
    "\n",
    "- **entry_date**: the day the signal becomes eligible (execution is T+1).  \n",
    "- **ticker**: asset identifier.  \n",
    "- **prob_up**: model probability used for sizing.  \n",
    "- **signal**: trade direction (+1 long, −1 short).  \n",
    "- **atr**: ATR value (used for stop distance and risk sizing).\n",
    "\n",
    "---\n",
    "\n",
    "### Why this helper matters\n",
    "\n",
    "- **Standardization**: ensures consistency between modeling outputs and the backtesting engine.  \n",
    "- **Flexibility**: by changing `prob_col` or passing a `gate_mask`, we can backtest base, final, or gated signals easily.  \n",
    "- **Risk-ready**: adds the ATR column directly, so the backtester can compute stop distances and position sizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5125dea2-fabc-4958-a982-b04fe8b8c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_merged(df_signals: pd.DataFrame,\n",
    "                df_features: pd.DataFrame,\n",
    "                prob_col: str,\n",
    "                t_star: float,\n",
    "                gate_mask: pd.Series | None = None,\n",
    "                atr_col: str = \"atr_10d_Tm1\") -> pd.DataFrame:\n",
    "   \n",
    "    df = df_signals.copy()\n",
    "    if gate_mask is not None:\n",
    "        df = df.loc[gate_mask].copy()\n",
    "\n",
    "   \n",
    "    df = (df.rename(columns={\"date\": \"entry_date\"})\n",
    "            .loc[:, [\"entry_date\", \"ticker\", prob_col]])\n",
    "    df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"]).dt.normalize()\n",
    "    df = df.rename(columns={prob_col: \"prob_up\"})\n",
    "    df[\"prob_up\"] = df[\"prob_up\"].astype(float)\n",
    "\n",
    "    \n",
    "    df[\"signal\"] = np.where(df[\"prob_up\"] >= t_star, 1, -1)\n",
    "\n",
    "    # ATR (T-1 in the set)\n",
    "    feat = df_features.loc[:, [\"date\", \"ticker\", atr_col]].copy()\n",
    "    feat[\"date\"] = pd.to_datetime(feat[\"date\"]).dt.normalize()\n",
    "    feat = feat.rename(columns={\"date\": \"entry_date\", atr_col: \"atr\"})\n",
    "\n",
    "    merged = (df.merge(feat, on=[\"entry_date\", \"ticker\"], how=\"left\")\n",
    "                .drop_duplicates(subset=[\"entry_date\", \"ticker\"])\n",
    "                .sort_values([\"entry_date\", \"ticker\"])\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "    \n",
    "    return merged[[\"entry_date\", \"ticker\", \"prob_up\", \"signal\", \"atr\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19276e-8996-4565-8300-860884d470a5",
   "metadata": {},
   "source": [
    "# Creating Different Versions of `merged_data`\n",
    "\n",
    "At this stage we have `df_signals` containing the model outputs:\n",
    "[‘date’, ‘ticker’, ‘prob_up_base’, ‘prob_up_final’, ‘meta_prob’, ‘signal’, …]\n",
    "and a decision threshold `t_star` that was chosen on the OOF (out-of-fold) probability curve.  \n",
    "We now use the helper `make_merged(...)` to prepare three different flavors of `merged_data` for backtesting.\n",
    "\n",
    "---\n",
    "\n",
    "## Variants\n",
    "\n",
    "- **`merged_base`**  \n",
    "  ```python\n",
    "  merged_base = make_merged(df_signals, df_ml, prob_col=\"prob_up_base\", t_star=t_star)\n",
    "Uses the raw base stacker probabilities (prob_up_base).\n",
    "This is the benchmark without any regime adjustment.\n",
    "- **`merged_final`**  \n",
    "  ```python\n",
    "  merged_final = make_merged(df_signals, df_ml, prob_col=\"prob_up_final\", t_star=t_star)\n",
    "Uses the regime-aware blended probabilities (prob_up_final).\n",
    "This incorporates the meta-classifier’s soft gating.\n",
    "- **`merged_gated`**  \n",
    "  ```python\n",
    "  merged_gated = make_merged(df_signals, df_ml,\n",
    "                           prob_col=\"prob_up_final\",\n",
    "                           t_star=t_star,\n",
    "                           gate_mask=(df_signals[\"meta_prob\"] >= TAU))\n",
    "Same as merged_final, but restricted to rows where the meta-classifier’s trust score meta_prob is above a threshold (TAU = 0.60).\n",
    "Effectively this is a hard-gated variant of the regime-aware strategy.\n",
    "## Why this is useful\n",
    "\n",
    "By generating **parallel versions** of `merged_data`, we can run the same backtest engine under different modeling assumptions:\n",
    "\n",
    "- **BASE**: pure model stack (no regime).  \n",
    "- **FINAL**: regime-aware soft blend.  \n",
    "- **GATED**: regime-aware but only active when the gate is confident.  \n",
    "\n",
    "This makes it easy to compare performance and evaluate whether the regime gate is truly adding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6d7371-6de7-4540-84d2-efbb1fd3311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE   entry_date ticker  prob_up  signal  atr\n",
      "0 2024-01-02   VYGR      0.5       1  NaN\n",
      "1 2024-01-04   ADCT      0.5       1  NaN\n",
      "FINAL   entry_date ticker  prob_up  signal  atr\n",
      "0 2024-01-02   VYGR      0.5       1  NaN\n",
      "1 2024-01-04   ADCT      0.5       1  NaN\n",
      "GATED   entry_date ticker   prob_up  signal      atr\n",
      "0 2025-03-26   ACTU  0.457329      -1  0.39513\n",
      "1 2025-03-26   ATAI  0.506686       1  0.10906\n"
     ]
    }
   ],
   "source": [
    "TAU = 0.60 \n",
    "merged_base  = make_merged(df_signals, df_ml, prob_col=\"prob_up_base\",  t_star=t_star)\n",
    "merged_final = make_merged(df_signals, df_ml, prob_col=\"prob_up_final\", t_star=t_star)\n",
    "merged_gated = make_merged(df_signals, df_ml, prob_col=\"prob_up_final\",\n",
    "                           t_star=t_star, gate_mask=(df_signals[\"meta_prob\"] >= TAU))\n",
    "\n",
    "# sanity check\n",
    "for name, dfm in [(\"BASE\", merged_base), (\"FINAL\", merged_final), (\"GATED\", merged_gated)]:\n",
    "    print(name, dfm.head(2))\n",
    "    assert {\"entry_date\",\"ticker\",\"prob_up\",\"signal\",\"atr\"} <= set(dfm.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879925bb-9593-4fb8-8355-4881a21c747c",
   "metadata": {},
   "source": [
    "# Probabilistic Signal → Portfolio Weights Helper\n",
    "\n",
    "### **Inputs**\n",
    "- **df**: `DataFrame` with at least `date`, `ticker`, and the probability column.\n",
    "- **prob_col** *(str)*: name of the probability column (e.g., `\"p\"`, `\"prob_up_final\"`).\n",
    "- **t** *(float, default = 0.0)*: confidence threshold around 0.5.  \n",
    "  Signals with \\(|prob - 0.5| < t\\) are set to 0 (ignored).\n",
    "- **long_only** *(bool, default = False)*:\n",
    "  - `False` → market-neutral normalization (longs + shorts).\n",
    "  - `True` → long-only normalization (shorts removed).\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "`DataFrame` with columns: [‘date’, ‘ticker’, ‘weight’]\n",
    "\n",
    "One row per asset-date.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scoring and Normalization Logic**\n",
    "\n",
    "#### 1. **Score (conviction relative to 0.5)**\n",
    "$$\n",
    "\\text{score}_i = \\text{prob}_i - 0.5\n",
    "$$\n",
    "\n",
    "- Positive scores → bullish convictions (long side).  \n",
    "- Negative scores → bearish convictions (short side).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Optional long-only constraint**\n",
    "If `long_only=True`, negative scores are clipped:\n",
    "$$\n",
    "\\text{score}_i \\leftarrow \\max(\\text{score}_i, 0)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Confidence threshold \\(t\\)**\n",
    "Weak signals ignored:\n",
    "$$\n",
    "\\text{score}_i =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } \\lvert \\text{score}_i \\rvert < t,\\\\\n",
    "\\text{score}_i & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Daily normalization (per date)**\n",
    "\n",
    "- **Market-neutral (default, `long_only=False`)**  \n",
    "  Sum of absolute weights = 1:\n",
    "  $$\n",
    "w_i=\\frac{\\mathrm{score}_i}{\\sum_j \\lvert \\mathrm{score}_j\\rvert+\\varepsilon}\n",
    "$$\n",
    "\n",
    "- **Long-only (`long_only=True`)**  \n",
    "  Sum of positive weights = 1:\n",
    "  $$\n",
    "w_i=\\frac{\\max\\!\\big(\\mathrm{score}_i,0\\big)}{\\sum_j \\max\\!\\big(\\mathrm{score}_j,0\\big)+\\varepsilon}\n",
    "$$\n",
    "\n",
    "- Small $\\varepsilon=10^{-12}$  ensures numerical safety when all scores = 0.\n",
    "\n",
    "---\n",
    "\n",
    "### **Behavior & Edge Cases**\n",
    "- **All scores zero** (e.g., tight threshold):  \n",
    "  Denominator ≈ \\(\\varepsilon\\), numerators = 0 → all weights = 0 (flat book).\n",
    "- **Single large conviction**:  \n",
    "  That asset ≈ weight ±1 (MN) or 1 (LO), others ≈ 0.\n",
    "- **Market-neutral**: negative weights represent short exposure.\n",
    "- **Long-only**: weights are non-negative and sum to 1 if any positive score; else all 0.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Helper is Practical**\n",
    "- Consistent **portfolio sizing** from probabilistic signals.  \n",
    "- Flexible posture:  \n",
    "  - **Market-neutral** (pairs-style) or **long-only** (benchmark-constrained) with one switch.  \n",
    "- **Confidence thresholding** suppresses noise & turnover.  \n",
    "- **Drop-in ready** for backtests: daily normalized weights simplify P&L attribution and risk checks.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fe6ed0-7dc2-437f-a239-1c09e3fd4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_weights(df, prob_col, t=0.0, long_only=False):\n",
    "    \n",
    "    s = (df[prob_col] - 0.5)\n",
    "    if long_only:\n",
    "        s = s.clip(lower=0.0)\n",
    "    s = s.where(s.abs() >= t, 0.0)  \n",
    "\n",
    "    df = df.assign(score=s)\n",
    "\n",
    "    def _normalize(g):\n",
    "        if long_only:\n",
    "            pos = g[\"score\"].clip(lower=0.0)\n",
    "            w = pos / (pos.sum() + 1e-12)\n",
    "        else:\n",
    "            w = g[\"score\"] / (g[\"score\"].abs().sum() + 1e-12)\n",
    "        return pd.DataFrame({\n",
    "            \"date\": g[\"date\"].values,\n",
    "            \"ticker\": g[\"ticker\"].values,\n",
    "            \"weight\": w.values\n",
    "        })\n",
    "\n",
    "   \n",
    "    out = df.groupby(\"date\", group_keys=False)[[\"date\",\"ticker\",\"score\"]].apply(_normalize)\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "signals_base   = to_weights(df_signals.rename(columns={\"prob_up_base\":\"p\"}),\n",
    "                            prob_col=\"p\", t=0.02, long_only=False)\n",
    "\n",
    "signals_final  = to_weights(df_signals.rename(columns={\"prob_up_final\":\"p\"}),\n",
    "                            prob_col=\"p\", t=0.02, long_only=False)\n",
    "\n",
    "\n",
    "TAU = 0.60\n",
    "mask_gate = df_signals[\"meta_prob\"] >= TAU\n",
    "signals_base_gated = to_weights(\n",
    "    df_signals.loc[mask_gate].rename(columns={\"prob_up_base\":\"p\"}),\n",
    "    prob_col=\"p\", t=0.02, long_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbda62c3-d788-432f-a272-2ad0cbf2cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices_ohlc(csv_path: str) -> pd.DataFrame:\n",
    "    px = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "    # normalizza\n",
    "    px = px.rename(columns={\n",
    "        \"Date\":\"date\",\"DATE\":\"date\",\n",
    "        \"Ticker\":\"ticker\",\"TICKER\":\"ticker\",\n",
    "        \"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\n",
    "        \"Adj Close\":\"adj_close\",\"adj_close\":\"adj_close\"\n",
    "    })\n",
    "    px[\"date\"] = pd.to_datetime(px[\"date\"]).dt.normalize()\n",
    "    px = px.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    have_open = \"open\" in px.columns\n",
    "    have_high = \"high\" in px.columns\n",
    "    have_low  = \"low\"  in px.columns\n",
    "    have_close= \"close\" in px.columns\n",
    "\n",
    "    if not have_close:\n",
    "        raise RuntimeError(\"file price without 'close' — at least (date,ticker,close).\")\n",
    "\n",
    "    \n",
    "    if not have_open:\n",
    "        px[\"open\"] = px.groupby(\"ticker\")[\"close\"].shift(1)\n",
    "        px[\"open\"] = px[\"open\"].fillna(px[\"close\"])\n",
    "\n",
    "    \n",
    "    if not have_high:\n",
    "        px[\"high\"] = px[[\"open\",\"close\"]].max(axis=1)\n",
    "    if not have_low:\n",
    "        px[\"low\"]  = px[[\"open\",\"close\"]].min(axis=1)\n",
    "\n",
    "    \n",
    "    prices = px[[\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\"]].copy()\n",
    "    prices = prices.dropna(subset=[\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\"])\n",
    "    prices = prices.drop_duplicates(subset=[\"date\",\"ticker\"])\n",
    "    return prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b893a489-44a6-4a6c-b120-fb1c8929cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ticker   open   high     low  close\n",
      "0 2025-02-13   AARD  15.18  15.18  11.550  14.31\n",
      "1 2025-02-14   AARD  14.34  14.43  12.905  13.30\n",
      "2 2025-02-18   AARD  13.39  14.40  12.580  13.56\n",
      "3 2025-02-19   AARD  13.54  16.48  13.425  14.90\n",
      "4 2025-02-20   AARD  15.89  19.58  15.630  17.25\n"
     ]
    }
   ],
   "source": [
    "prices = load_prices_ohlc(p(\"underlying_prices_complete.csv\"))\n",
    "print(prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0062ca95-e0de-44e6-b74f-465724c79fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUTDIR = Path(\"bt_runs\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_bt(name: str,\n",
    "            port: pd.DataFrame,\n",
    "            trades: pd.DataFrame,\n",
    "            miss: pd.DataFrame | None,\n",
    "            metrics: dict,\n",
    "            args: dict,\n",
    "            outdir: Path = OUTDIR) -> None:\n",
    "   \n",
    "    \n",
    "    eq = metrics[\"Equity\"]\n",
    "   \n",
    "    port_to_save = port.copy()\n",
    "    if port_to_save.index.name is None:\n",
    "        port_to_save.index.name = \"date\"\n",
    "    port_to_save.reset_index().to_csv(outdir / f\"portfolio_equity_{name}.csv\", index=False)\n",
    "\n",
    "    \n",
    "    (trades if trades is not None else pd.DataFrame()).to_csv(\n",
    "        outdir / f\"trading_history_{name}.csv\", index=False\n",
    "    )\n",
    "\n",
    "   \n",
    "    if miss is not None and len(miss):\n",
    "        miss.to_csv(outdir / f\"missing_prices_{name}.csv\", index=False)\n",
    "\n",
    "   \n",
    "    row = {\n",
    "        \"run\": name,\n",
    "        \"cagr\": float(metrics.get(\"CAGR\", np.nan)),\n",
    "        \"sharpe\": float(metrics.get(\"Sharpe\", np.nan)),\n",
    "        \"max_dd\": float(metrics.get(\"Max DD\", np.nan)),\n",
    "        \"win_rate\": float(metrics.get(\"Win rate\", np.nan)) if metrics.get(\"Num trades\", 0) > 0 else np.nan,\n",
    "        \"profit_factor\": float(metrics.get(\"Profit factor\", np.nan)) if metrics.get(\"Num trades\", 0) > 0 else np.nan,\n",
    "        \"num_trades\": int(metrics.get(\"Num trades\", 0)),\n",
    "        \"tot_return\": float(eq.iloc[-1] / eq.iloc[0] - 1) if len(eq) else np.nan,\n",
    "    }\n",
    "    summ_path = outdir / \"metrics_summary.csv\"\n",
    "    if summ_path.exists():\n",
    "        summary = pd.read_csv(summ_path)\n",
    "        summary = pd.concat([summary, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        summary = pd.DataFrame([row])\n",
    "    summary.to_csv(summ_path, index=False)\n",
    "\n",
    "   \n",
    "    (outdir / \"last_bt_args.json\").write_text(json.dumps(args, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f0f5dd-e0b5-42e0-96c6-6ac038038dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> BASE …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back-test: 100%|████████████████████████████| 551/551 [00:00<00:00, 1701.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> FINAL …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back-test: 100%|████████████████████████████| 551/551 [00:00<00:00, 1437.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> GATED …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1463.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY TABLE ===\n",
      "  run      CAGR  Sharpe(ann)    Max DD  Win rate  Profit factor  #trades    TotRet\n",
      " BASE -0.194505    -1.074737 -0.302458  0.447900       0.861867      643 -0.277982\n",
      "FINAL -0.193466    -1.720422 -0.301102  0.471983       0.930198      928 -0.276579\n",
      "GATED  0.450552     1.206264 -0.092459  0.530864       1.236173      405  0.108326\n",
      "\n",
      "Expectancy per trade (BASE) : -0.6563%\n",
      "Expectancy per trade (FINAL): -0.3203%\n",
      "Expectancy per trade (GATED): 0.9820%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "bt_args = dict(\n",
    "    prices=prices,\n",
    "    prob_long_min = float(round(0.489, 3)),  \n",
    "    prob_short_min= float(round(0.489, 3)),   \n",
    "    commission_pct=0.001,                      \n",
    "    slippage_pct =0.004,                       \n",
    "    initial_capital=100000\n",
    ")\n",
    "# backtest \n",
    "print(\">>> BASE …\")\n",
    "port_b, trades_b, miss_b = run_backtest(merged_base,  **bt_args)\n",
    "\n",
    "print(\">>> FINAL …\")\n",
    "port_f, trades_f, miss_f = run_backtest(merged_final, **bt_args)\n",
    "\n",
    "print(\">>> GATED …\")\n",
    "port_g, trades_g, miss_g = run_backtest(merged_gated, **bt_args)\n",
    "\n",
    "\n",
    "def expectancy(trades_df: pd.DataFrame) -> float:\n",
    "    if trades_df.empty:\n",
    "        return float(\"nan\")\n",
    "    wr = (trades_df[\"return\"] > 0).mean()\n",
    "    avg_win  = trades_df.loc[trades_df[\"return\"] > 0, \"return\"].mean()\n",
    "    avg_loss = -trades_df.loc[trades_df[\"return\"] < 0, \"return\"].mean()\n",
    "    return wr*avg_win - (1-wr)*avg_loss\n",
    "\n",
    "m_b = calculate_metrics(port_b, trades_b)\n",
    "m_f = calculate_metrics(port_f, trades_f)\n",
    "m_g = calculate_metrics(port_g, trades_g)\n",
    "\n",
    "print(\"\\n=== SUMMARY TABLE ===\")\n",
    "import pandas as pd, numpy as np\n",
    "def row(name, m):\n",
    "    eq = m[\"Equity\"]\n",
    "    tot_ret = float(eq.iloc[-1]/eq.iloc[0] - 1) if len(eq) else np.nan\n",
    "    return {\n",
    "        \"run\": name,\n",
    "        \"CAGR\": m[\"CAGR\"], \"Sharpe(ann)\": m[\"Sharpe\"], \"Max DD\": m[\"Max DD\"],\n",
    "        \"Win rate\": m[\"Win rate\"], \"Profit factor\": m[\"Profit factor\"],\n",
    "        \"#trades\": m[\"Num trades\"], \"TotRet\": tot_ret\n",
    "    }\n",
    "summary_df = pd.DataFrame([row(\"BASE\", m_b), row(\"FINAL\", m_f), row(\"GATED\", m_g)])\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nExpectancy per trade (BASE) : {expectancy(trades_b):.4%}\")\n",
    "print(f\"Expectancy per trade (FINAL): {expectancy(trades_f):.4%}\")\n",
    "print(f\"Expectancy per trade (GATED): {expectancy(trades_g):.4%}\")\n",
    "save_bt(\"BASE\",  port_b, trades_b, miss_b, m_b, bt_args)\n",
    "save_bt(\"FINAL\", port_f, trades_f, miss_f, m_f, bt_args)\n",
    "save_bt(\"GATED\", port_g, trades_g, miss_g, m_g, bt_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f3112-c674-4105-afee-a5d5afbc79e6",
   "metadata": {},
   "source": [
    "### Backtest results: BASE vs FINAL vs GATED \n",
    "**Costs (net): commission = 10 bps per side, slippage = 40 bps per side → ~50 bps per side, ~100 bps round-trip. Applied on both entry and exit.**\n",
    "### Key takeaways (updated)\n",
    "\n",
    "**BASE and FINAL** (always-on models) are negative over the sample:\n",
    "- **TotRet:** ≈ **−27.8%** (BASE) / **−27.6%** (FINAL)  \n",
    "- **Sharpe:** ≈ **−1.07** / **−1.72**  \n",
    "- **MaxDD:** ≈ **−30.2%** / **−30.1%**  \n",
    "- **Activity:** **643** / **928** trades with weak **expectancy per trade** (**−0.65%** / **−0.32%**)\n",
    "\n",
    "**GATED** (regime-aware, hard filter) is materially better:\n",
    "- **TotRet:** ≈ **+10.8%**  \n",
    "- **Sharpe:** ≈ **1.20**  \n",
    "- **MaxDD:** ≈ **−9.2%**  \n",
    "- **Win rate:** ≈ **53%** · **Profit factor:** ≈ **1.24**  \n",
    "- **#trades:** **405** (the gate blocks low-quality regimes and concentrates exposure when the meta-signal is confident)  \n",
    "- **Expectancy per trade:** ≈ **+0.982%**\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- The **gate** reduces the number of trades and **avoids the deepest drawdowns** seen in BASE/FINAL.  \n",
    "- **Positive expectancy per trade** rises to ~**0.982%** (vs **−0.65%** BASE / **−0.32%** FINAL), indicating the regime filter **selects higher-quality opportunities** and improves risk efficiency.\n",
    "\n",
    "--- \n",
    "## Disclosures (brief)\n",
    "\n",
    "Costs model: constant % slippage by side (40 bps) + commission (10 bps) — a simplification suitable for sensitivity analysis; real costs depend on spread, depth, volatility, and participation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78c179-72ca-4eba-8533-67fcb9f665ae",
   "metadata": {},
   "source": [
    "> 🚩 **Note — Regime features update**\n",
    ">\n",
    "> **Regime gate change:** added `IBB_v_Tm1`, removed `spread_3m_10y` and modified principal features list.  \n",
    "> **AUC:** **~0.57–0.58 OOF** and **~0.626 in 2025Q2**.  \n",
    "> **P&L impact:** Worst results due to **under-calibrated probabilities**.  \n",
    "> **Next steps:** apply **Platt scaling** and **isotonic regression** to fix calibration & thresholding.  \n",
    "> **For this report:** highlight the **earlier configuration** (stronger realized **P&L**) while calibration upgrades are finalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfb83a-ad94-49f3-a689-c11078b567f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### STRESS-TEST (COSTS - SLIPPAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4a0c31-6629-46b4-b20a-20348ffaaf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1372.07it/s]\n",
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1437.19it/s]\n",
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1540.26it/s]\n",
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1482.12it/s]\n",
      "Back-test: 100%|████████████████████████████| 102/102 [00:00<00:00, 1508.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slippage</th>\n",
       "      <th>CAGR</th>\n",
       "      <th>Sharpe</th>\n",
       "      <th>MaxDD</th>\n",
       "      <th>PF</th>\n",
       "      <th>WinRate</th>\n",
       "      <th>#trades</th>\n",
       "      <th>Expect/trade</th>\n",
       "      <th>TotRet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.365363</td>\n",
       "      <td>1.028827</td>\n",
       "      <td>-0.099453</td>\n",
       "      <td>1.183920</td>\n",
       "      <td>0.530864</td>\n",
       "      <td>405</td>\n",
       "      <td>0.00782</td>\n",
       "      <td>0.089931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.285254</td>\n",
       "      <td>0.851048</td>\n",
       "      <td>-0.106384</td>\n",
       "      <td>1.133920</td>\n",
       "      <td>0.528395</td>\n",
       "      <td>405</td>\n",
       "      <td>0.00582</td>\n",
       "      <td>0.071860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.209920</td>\n",
       "      <td>0.673070</td>\n",
       "      <td>-0.113254</td>\n",
       "      <td>1.086014</td>\n",
       "      <td>0.520988</td>\n",
       "      <td>405</td>\n",
       "      <td>0.00382</td>\n",
       "      <td>0.054105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.072435</td>\n",
       "      <td>0.317093</td>\n",
       "      <td>-0.126813</td>\n",
       "      <td>0.996105</td>\n",
       "      <td>0.516049</td>\n",
       "      <td>405</td>\n",
       "      <td>-0.00018</td>\n",
       "      <td>0.019526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.139381</td>\n",
       "      <td>-0.133503</td>\n",
       "      <td>0.953915</td>\n",
       "      <td>0.513580</td>\n",
       "      <td>405</td>\n",
       "      <td>-0.00218</td>\n",
       "      <td>0.002689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   slippage      CAGR    Sharpe     MaxDD        PF   WinRate  #trades  \\\n",
       "0     0.005  0.365363  1.028827 -0.099453  1.183920  0.530864      405   \n",
       "1     0.006  0.285254  0.851048 -0.106384  1.133920  0.528395      405   \n",
       "2     0.007  0.209920  0.673070 -0.113254  1.086014  0.520988      405   \n",
       "3     0.009  0.072435  0.317093 -0.126813  0.996105  0.516049      405   \n",
       "4     0.010  0.009760  0.139381 -0.133503  0.953915  0.513580      405   \n",
       "\n",
       "   Expect/trade    TotRet  \n",
       "0       0.00782  0.089931  \n",
       "1       0.00582  0.071860  \n",
       "2       0.00382  0.054105  \n",
       "3      -0.00018  0.019526  \n",
       "4      -0.00218  0.002689  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = [0.005, 0.006,0.007, 0.009,0.01]\n",
    "rows = []\n",
    "for s in grid:\n",
    "    bt_args2 = {**bt_args, \"slippage_pct\": s}\n",
    "    port, trades, miss = run_backtest(merged_gated, **bt_args2)  #  GATED\n",
    "    m = calculate_metrics(port, trades)\n",
    "    eq = m[\"Equity\"]\n",
    "    tot_ret = float(eq.iloc[-1]/eq.iloc[0]-1) if len(eq) else float(\"nan\")\n",
    "    rows.append({\n",
    "        \"slippage\": s,\n",
    "        \"CAGR\": m[\"CAGR\"],\n",
    "        \"Sharpe\": m[\"Sharpe\"],\n",
    "        \"MaxDD\": m[\"Max DD\"],\n",
    "        \"PF\": m[\"Profit factor\"],\n",
    "        \"WinRate\": m[\"Win rate\"],\n",
    "        \"#trades\": m[\"Num trades\"],\n",
    "       \n",
    "        \"Expect/trade\": (trades[\"return\"].mean() if len(trades) else float(\"nan\")),\n",
    "        \"TotRet\": tot_ret\n",
    "    })\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd67735-bf96-4751-81f3-9c98250e354e",
   "metadata": {},
   "source": [
    "### - Slippage sensitivity (per side: entry and exit)\n",
    "\n",
    "### How to read it \n",
    "\n",
    "- Increasing slippage per side from **50 bps → 100 bps** steadily erodes **CAGR, Sharpe, Profit Factor,** and **expectancy per trade**.  \n",
    "- **Break-even zone:** around **0.9% per side** the Profit Factor ≈ 1.0 and expectancy ~ 0%, i.e., the edge is essentially consumed.  \n",
    "- Win rate drifts down slightly; **#trades stays constant** (same signals; only costs change).\n",
    "\n",
    "> This presentation fits small-cap biotech; pair with a short note on **liquidity filters** and **participation limits** to justify base and stress levels.\n",
    "m\n",
    "### Scenario labels used in the study (small-cap biotech universe)\n",
    "- **Base - positive(off-table):** 0.004 (Tot ret = 10.8% | Max DD = -9.2%)\n",
    "- **Base – conservative:** 0.005 and 0.006 per side  (Tot ret = 9% | 7.2% , Max DD = -10% | -10.6%)\n",
    "- **Stress:** 0.007 per side   (Tot ret = 5.4% , Max DD = -11.3%)\n",
    "- **Stress – high:** 0.009 per side  (Tot ret = 1.9% , Max DD = -12.7%)\n",
    "- **Stress – very high:** 0.010 per side  ((Tot ret = 0.26% , Max DD = -13.3%)\n",
    "\n",
    "These tiers reflect realistic-to-conservative execution assumptions for the segment. As slippage rises, **CAGR/Sharpe/PF decline**, and **expectancy approaches or breaches zero** around **~0.009–0.010 per side**.\n",
    "\n",
    "-**All scenarios include commission = 10 bps per side; the table varies slippage per side only. Results are net of costs. Per side = entry or exit; round-trip ≈ 2× per-side.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a72a6f-1f24-4555-afca-4dd286bf3631",
   "metadata": {},
   "source": [
    "#### Why higher slippage hurts more when you trade more\n",
    "\n",
    "- P&L erosion is roughly **proportional to the number of trades** (and traded notional).  \n",
    "- More trades ⇒ more times you “pay” the per-side cost ⇒ **CAGR, Sharpe, and Profit Factor decline** as slippage rises.  \n",
    "- This matches the **monotonic drop** observed in the table.\n",
    "\n",
    "#### Model-development outlook (reducing slippage sensitivity)\n",
    "\n",
    "We expect similar baseline performance but **lower sensitivity to slippage** by trading **less but higher-quality**:\n",
    "\n",
    "- **Probability calibration:** Platt / Isotonic → better-calibrated scores → tighter, confidence-based filters → **fewer marginal trades**.  \n",
    "- **Stacking upgrade:** add **ElasticNet** in the meta-learner → improved generalization and **sparser decision boundaries**.  \n",
    "- **\"Shadow model\" with higher AUC:** use it for **meta-gating or consensus** → engage only when agreement/confidence is high.\n",
    "\n",
    "**Net effect:** fewer, more selective trades, **higher average edge per trade**, and **less performance decay** as slippage increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a6295-e49b-454d-8c8c-5f5ff7391413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
