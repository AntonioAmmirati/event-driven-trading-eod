{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004c70d1-84f3-4b32-9495-a096bf9989d9",
   "metadata": {},
   "source": [
    "#### 01. News Ingestion (Import & Setup)\n",
    "\n",
    "This pipeline pulls biotech news from **Google News RSS** and the **Finnhub API** over the period **2024‑02‑01 → 2025‑07‑01** for our biotech small‑cap universe.\n",
    "\n",
    "- **Google RSS**: fetches titles + summaries, filtered by keywords (biotech, FDA, clinical trial).  \n",
    "- **Finnhub**: company‑news endpoint with additional regex filtering for approvals, clinical phases, partnerships, etc.  \n",
    "- Finally we’ll merge the two sets into ~6,791 unique `(ticker, date, title)` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d00ced-c75e-4c8b-b025-10c9b563a9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path.cwd()\n",
    "DATA_DIR = Path(os.getenv(\"DATA_DIR\", ROOT / \"data\"))  \n",
    "def p(file): return DATA_DIR / file\n",
    "load_dotenv(find_dotenv(usecwd=True), override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbec97a-1468-4e67-92f4-5fc16d78e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "import re\n",
    "import time, re, gc, math, random\n",
    "from datetime import datetime,timedelta\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "ml      = pd.read_parquet(p(\"ML.parquet\"))\n",
    "tickers = ml[\"ticker\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eceed39-63b5-4744-b3c9-336794e19e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "start_dt = datetime(2024,1,1)\n",
    "end_dt   = datetime(2025,7,30)\n",
    "\n",
    "\n",
    "BASE_RSS = \"https://news.google.com/rss/search?q=\"\n",
    "records  = []\n",
    "\n",
    "for tk in tickers:\n",
    "   \n",
    "    q   = quote(f'\"{tk}\" biotech OR FDA OR \"clinical trial\"')\n",
    "    url = f\"{BASE_RSS}{q}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    \n",
    "    feed = feedparser.parse(url)\n",
    "    for e in feed.entries:\n",
    "        \n",
    "        if hasattr(e, \"published_parsed\") and e.published_parsed:\n",
    "            dt = datetime.fromtimestamp(mktime(e.published_parsed))\n",
    "        else:\n",
    "            dt = pd.to_datetime(e.get(\"published\", None), errors=\"coerce\")\n",
    "        \n",
    "        if not pd.isna(dt) and start_dt <= dt <= end_dt:\n",
    "            records.append({\n",
    "                \"ticker\":      tk,\n",
    "                \"title\":       e.get(\"title\"),\n",
    "                \"summary\":     e.get(\"summary\"),\n",
    "                \"link\":        e.get(\"link\"),\n",
    "                \"publishedAt\": dt\n",
    "            })\n",
    "   \n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "df_news = pd.DataFrame(records)\n",
    "print(\"Articoli totali nel periodo:\", len(df_news))\n",
    "print(df_news.head())\n",
    "\n",
    "df_news.to_parquet(\"biotech_google_news_2024_20250627.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a06b6-d5d0-4323-a698-c8795ad86ca8",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "Total: 4121\n",
    "  ticker                                              title  \\\n",
    "0   CDXS  What analysts say about CDXS stock - Remarkabl...   \n",
    "1   CDXS  Codexis (NASDAQ:CDXS investor three-year losse...   \n",
    "2   CDXS  Opaleye Management Inc. Increases Stake in Cod...   \n",
    "3   CDXS  Codexis, Inc. (NASDAQ:CDXS) Just Released Its ...   \n",
    "4   CDXS  Casdin Capital, LLC Increases Stake in Codexis...   \n",
    "\n",
    "                                             summary  \\\n",
    "0  <a href=\"https://news.google.com/rss/articles/...   \n",
    "1  <a href=\"https://news.google.com/rss/articles/...   \n",
    "2  <a href=\"https://news.google.com/rss/articles/...   \n",
    "3  <a href=\"https://news.google.com/rss/articles/...   \n",
    "4  <a href=\"https://news.google.com/rss/articles/...   \n",
    "\n",
    "                                                link         publishedAt  \n",
    "0  https://news.google.com/rss/articles/CBMigwFBV... 2025-07-25 20:04:52  \n",
    "1  https://news.google.com/rss/articles/CBMi1wFBV... 2025-03-01 08:00:00  \n",
    "2  https://news.google.com/rss/articles/CBMijAFBV... 2024-09-20 08:00:00  \n",
    "3  https://news.google.com/rss/articles/CBMi2AFBV... 2025-05-17 08:00:00  \n",
    "4  https://news.google.com/rss/articles/CBMihwFBV... 2024-09-26 08:00:00  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e79301-91de-4c67-8ba5-cc5e4c0e2a9e",
   "metadata": {},
   "source": [
    "#  2. Google News RSS\n",
    "\n",
    "For each ticker:\n",
    "- build the query `\"TICKER\" biotech OR FDA OR \"clinical trial\"`,  \n",
    "- download the RSS feed,  \n",
    "- extract title, summary, link, and publication date,  \n",
    "- respect a 2 s delay to avoid overloading Google,  \n",
    "- cache results on disk for faster iterations during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913b266-e240-43cf-87cc-30c145a5f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_list = ml[\"ticker\"].unique().tolist()\n",
    "BASE_URL = os.getenv(\"BASE_URL\", \"https://news.google.com/rss/search?q=\")\n",
    "API_KEY  = os.getenv(\"API_KEY\")   \n",
    "FROM_DATE = datetime(2024,1,1)\n",
    "TO_DATE   = datetime(2025,7,30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_REQ_PER_MIN = 55\n",
    "PAUSE_SEC       = 60 / MAX_REQ_PER_MIN      \n",
    "\n",
    "CHUNK_SIZE  = 25    \n",
    "CHUNK_PAUSE = 30     \n",
    "\n",
    "\n",
    "kw_pattern = (\n",
    "    r\"\\bapproval\\b\"\n",
    "    r\"| \\bphase\\s*(?:I|II|III)\\b\"\n",
    "    r\"| \\btrial\\b\"\n",
    "    r\"| \\borphan\\b\"\n",
    "    r\"| \\bbreakthrough\\b\"\n",
    "    r\"| \\blicense\\b\"\n",
    "    r\"| \\bdeal\\b\"\n",
    "    r\"| \\bacqui(?:sition|re)\\b\"\n",
    "    r\"| \\bpartner(?:ship)?\\b\"\n",
    "    r\"| \\bFDA\\b\"\n",
    ")\n",
    "kw_re = re.compile(kw_pattern, flags=re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date, step_days=90):\n",
    "    cur = start_date\n",
    "    while cur <= end_date:\n",
    "        nxt = min(cur + timedelta(days=step_days - 1), end_date)\n",
    "        yield cur, nxt\n",
    "        cur = nxt + timedelta(days=1)\n",
    "\n",
    "\n",
    "def _build_session() -> requests.Session:\n",
    "    sess = requests.Session()\n",
    "    sess.headers[\"User-Agent\"] = \"biotech-news-screener/0.2\"\n",
    "\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,              \n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    sess.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return sess\n",
    "\n",
    "SESSION = _build_session()\n",
    "\n",
    "\n",
    "def fetch_company_news(ticker: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for d0, d1 in daterange(FROM_DATE, TO_DATE, step_days=90):\n",
    "        params = {\n",
    "            \"symbol\": ticker,\n",
    "            \"from\":   d0.date().isoformat(),\n",
    "            \"to\":     d1.date().isoformat(),\n",
    "            \"token\":  API_KEY,\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        resp = SESSION.get(\n",
    "            BASE_URL,\n",
    "            params=params,\n",
    "            timeout=(3, 30)        \n",
    "        )\n",
    "        if resp.status_code != 200:\n",
    "           \n",
    "            raise RuntimeError(f\"{ticker} HTTP {resp.status_code}\")\n",
    "\n",
    "        data = resp.json() or []\n",
    "        if not data:\n",
    "           \n",
    "            time.sleep(max(0, PAUSE_SEC - (time.time() - t0)))\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"datetime\"], unit=\"s\").dt.date\n",
    "        df = df[(df[\"date\"] >= d0.date()) & (df[\"date\"] <= d1.date())]\n",
    "\n",
    "        if not df.empty:\n",
    "            mask = (\n",
    "                df[\"headline\"].str.contains(kw_re, na=False)\n",
    "                | df[\"summary\"].str.contains(kw_re, na=False)\n",
    "            )\n",
    "            df = df[mask]\n",
    "            if not df.empty:\n",
    "                frames.append(\n",
    "                    df.loc[:, [\"date\", \"headline\", \"source\", \"url\"]]\n",
    "                      .rename(columns={\"headline\": \"title\"})\n",
    "                      .assign(ticker=ticker)\n",
    "                )\n",
    "\n",
    "       \n",
    "        time.sleep(max(0, PAUSE_SEC - (time.time() - t0)))\n",
    "\n",
    "    return (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "          .drop_duplicates(subset=[\"ticker\", \"date\", \"title\"])\n",
    "    ) if frames else pd.DataFrame()\n",
    "\n",
    "\n",
    "def scrape_all(tickers: list[str]) -> pd.DataFrame:\n",
    "    all_events = []\n",
    "    for i, tk in enumerate(tickers, 1):\n",
    "        try:\n",
    "            all_events.append(fetch_company_news(tk))\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}/{len(tickers)}] {tk}  ERRORE: {e}\")\n",
    "\n",
    "        \n",
    "        if i % CHUNK_SIZE == 0:\n",
    "            time.sleep(CHUNK_PAUSE)\n",
    "\n",
    "    df_all = (\n",
    "        pd.concat(all_events, ignore_index=True)\n",
    "          .drop_duplicates(subset=[\"ticker\", \"date\", \"title\"])\n",
    "          .sort_values([\"ticker\", \"date\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return df_all\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers_list = ml[\"ticker\"].unique().tolist()    \n",
    "    df_all = scrape_all(tickers_list)\n",
    "    print(\"Totale eventi raccolti:\", len(df_all))\n",
    "    df_all.to_parquet(\"biotech_finnhub_events.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df190054-0832-437d-be72-b50d460a3e29",
   "metadata": {},
   "source": [
    "#  3. Finnhub Company News\n",
    "\n",
    "To enrich with **headline  filtered by biotech keywords:\n",
    "- split the date range into 30‑day blocks  \n",
    "- hit the **company-news** endpoint  \n",
    "- apply a regex filter for approval, clinical phase, partnership keywords  \n",
    "- implement a back‑off if we hit a `429` rate limit  \n",
    "- cache each ticker’s results on disk  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8fc073-6090-4135-b8aa-7ec840074568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Duplicated 1709/8501  (20.1%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>finnhub_title</th>\n",
       "      <th>ticker</th>\n",
       "      <th>rss_titles</th>\n",
       "      <th>rss_summaries</th>\n",
       "      <th>link</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>all_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>2025-07-17</td>\n",
       "      <td></td>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks Sets Q2 Earnings Date, Announces Maj...</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMivgFBV...</td>\n",
       "      <td>2025-07-17 11:00:00</td>\n",
       "      <td>Zymeworks Sets Q2 Earnings Date, Announces Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>2025-07-27</td>\n",
       "      <td></td>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks (NASDAQ:ZYME) Stock Rating Upgraded ...</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiqgFBV...</td>\n",
       "      <td>2025-07-27 07:28:34</td>\n",
       "      <td>Zymeworks (NASDAQ:ZYME) Stock Rating Upgraded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Zymeworks Announces FDA Clearance of Investiga...</td>\n",
       "      <td>ZYME</td>\n",
       "      <td>FDA Green Lights Revolutionary First-in-Class ...</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMitwFBV...</td>\n",
       "      <td>2025-07-28 11:00:00</td>\n",
       "      <td>Zymeworks Announces FDA Clearance of Investiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td></td>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks (ZYME) Is Down 6.0% After FDA Clears...</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMi2wFBV...</td>\n",
       "      <td>2025-07-29 12:04:57</td>\n",
       "      <td>Zymeworks (ZYME) Is Down 6.0% After FDA Clear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6791</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td></td>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks' IND for liver cancer antibody-drug ...</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiwwFBV...</td>\n",
       "      <td>2025-07-29 18:25:41</td>\n",
       "      <td>Zymeworks' IND for liver cancer antibody-drug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                      finnhub_title ticker  \\\n",
       "6787 2025-07-17                                                      ZYME   \n",
       "6788 2025-07-27                                                      ZYME   \n",
       "6789 2025-07-28  Zymeworks Announces FDA Clearance of Investiga...   ZYME   \n",
       "6790 2025-07-29                                                      ZYME   \n",
       "6791 2025-07-29                                                      ZYME   \n",
       "\n",
       "                                             rss_titles  \\\n",
       "6787  Zymeworks Sets Q2 Earnings Date, Announces Maj...   \n",
       "6788  Zymeworks (NASDAQ:ZYME) Stock Rating Upgraded ...   \n",
       "6789  FDA Green Lights Revolutionary First-in-Class ...   \n",
       "6790  Zymeworks (ZYME) Is Down 6.0% After FDA Clears...   \n",
       "6791  Zymeworks' IND for liver cancer antibody-drug ...   \n",
       "\n",
       "                                          rss_summaries  \\\n",
       "6787  <a href=\"https://news.google.com/rss/articles/...   \n",
       "6788  <a href=\"https://news.google.com/rss/articles/...   \n",
       "6789  <a href=\"https://news.google.com/rss/articles/...   \n",
       "6790  <a href=\"https://news.google.com/rss/articles/...   \n",
       "6791  <a href=\"https://news.google.com/rss/articles/...   \n",
       "\n",
       "                                                   link         publishedAt  \\\n",
       "6787  https://news.google.com/rss/articles/CBMivgFBV... 2025-07-17 11:00:00   \n",
       "6788  https://news.google.com/rss/articles/CBMiqgFBV... 2025-07-27 07:28:34   \n",
       "6789  https://news.google.com/rss/articles/CBMitwFBV... 2025-07-28 11:00:00   \n",
       "6790  https://news.google.com/rss/articles/CBMi2wFBV... 2025-07-29 12:04:57   \n",
       "6791  https://news.google.com/rss/articles/CBMiwwFBV... 2025-07-29 18:25:41   \n",
       "\n",
       "                                             all_titles  \n",
       "6787   Zymeworks Sets Q2 Earnings Date, Announces Ma...  \n",
       "6788   Zymeworks (NASDAQ:ZYME) Stock Rating Upgraded...  \n",
       "6789  Zymeworks Announces FDA Clearance of Investiga...  \n",
       "6790   Zymeworks (ZYME) Is Down 6.0% After FDA Clear...  \n",
       "6791   Zymeworks' IND for liver cancer antibody-drug...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin = pd.read_parquet(\"biotech_finnhub_events.parquet\")\n",
    "df_news = pd.read_parquet(\"biotech_google_news_2024_20250627.parquet\")\n",
    "df_fin = df_fin.drop(['source','url'],axis =1)\n",
    "df_fin['ticker'] = df_fin['ticker'].str.upper()\n",
    "df_fin['date'] = pd.to_datetime(df_fin['date']).dt.normalize()\n",
    "df_news['date'] = df_news['publishedAt']\n",
    "df_news.columns = ['ticker' , 'rss_titles' , 'rss_summaries', 'link' , 'publishedAt' , 'date']\n",
    "df_fin.columns = ['date','finnhub_title','ticker']\n",
    "\n",
    "df_news['ticker'] = df_news['ticker'].str.upper()\n",
    "df_news['date'] = pd.to_datetime(df_news['date']).dt.normalize()\n",
    "\n",
    "\n",
    "df_all = pd.merge(\n",
    "    df_fin,\n",
    "    df_news,\n",
    "    on=['ticker', 'date'],\n",
    "    how='outer',\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "def _normalize(txt: str) -> str:\n",
    "    txt = str(txt).lower()\n",
    "    txt = re.sub(r'[^a-z0-9]+', ' ', txt)        \n",
    "    return re.sub(r'\\s+', ' ', txt).strip()\n",
    "\n",
    "df_all['canonical_title'] = (\n",
    "      df_all['finnhub_title'].str.strip().replace('', pd.NA)\n",
    "        .fillna(df_all['rss_titles'].str.strip())\n",
    "        .fillna('')\n",
    "        .apply(_normalize)\n",
    ")\n",
    "\n",
    "\n",
    "dup_mask = df_all.duplicated(\n",
    "    subset=['ticker', 'date', 'canonical_title'],\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "n_total = len(df_all)\n",
    "n_dups  = dup_mask.sum()\n",
    "print(f\"Duplicated {n_dups}/{n_total}  ({n_dups/n_total:.1%})\")\n",
    "\n",
    "\n",
    "df_all = df_all.loc[~dup_mask].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_all['finnhub_title'] = df_all['finnhub_title'].fillna('')\n",
    "df_all['rss_titles']    = df_all['rss_titles']   .fillna('')\n",
    "df_all['rss_summaries'] = df_all['rss_summaries'].fillna('')\n",
    "\n",
    "df_all['all_titles'] = df_all['finnhub_title'] + ' ' + df_all['rss_titles']\n",
    "\n",
    "\n",
    "df_all = df_all.drop(columns=['canonical_title'])\n",
    "\n",
    "\n",
    "df_all.to_csv('news_merged_dedup.csv', index=False)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe4c5d-1f84-467e-9af7-355bdc9ec1f7",
   "metadata": {},
   "source": [
    "#  4. Merge RSS + Finnhub\n",
    "\n",
    "We join the two datasets on `(ticker, date)`, keep all rows, and create an `all_titles = rss_title + \" ; \" + finn_title` field for downstream sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f355133-4642-4aa4-ab20-92015f2b45ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
